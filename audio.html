<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How AI Hears Audio</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Helvetica', 'Arial', sans-serif;
            background: #ffffff;
            margin: 0;
            padding: 20px;
            color: #333;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            background: #ffffff;
        }

        h1 {
            text-align: center;
            margin-bottom: 30px;
            font-size: 2em;
            color: #333;
        }

        .description {
            text-align: center;
            color: #666;
            margin-bottom: 30px;
            font-size: 16px;
            line-height: 1.5;
        }

        .audio-section {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
        }

        .section-title {
            font-weight: bold;
            margin-bottom: 15px;
            color: #333;
            font-size: 18px;
        }

        .audio-player {
            width: 100%;
            margin-bottom: 15px;
        }

        .audio-info {
            font-size: 14px;
            color: #666;
            text-align: center;
        }

        .waveform-container {
            background: #2d3748;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            height: 200px;
            position: relative;
            overflow: hidden;
        }

        .waveform-canvas {
            width: 100%;
            height: 100%;
            background: #1a202c;
            border-radius: 4px;
        }

        .spectrogram-container {
            background: #2d3748;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            height: 300px;
            position: relative;
            overflow: hidden;
        }

        .spectrogram-canvas {
            width: 100%;
            height: 100%;
            background: #1a202c;
            border-radius: 4px;
        }

        .token-section {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
        }

        .token-array {
            background: #2d3748;
            color: #e2e8f0;
            border: 1px solid #4a5568;
            border-radius: 8px;
            padding: 15px;
            font-family: 'Monaco', 'Menlo', 'Consolas', monospace;
            font-size: 13px;
            line-height: 1.4;
            overflow-x: auto;
            white-space: pre-wrap;
            word-break: break-all;
            margin-bottom: 15px;
        }

        .token-explanation {
            font-size: 14px;
            color: #666;
            background: #f0f8ff;
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #007bff;
        }

        .progress-indicator {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 20px 0;
            font-size: 14px;
            color: #666;
        }

        .progress-step {
            display: flex;
            flex-direction: column;
            align-items: center;
            flex: 1;
            position: relative;
        }

        .progress-step:not(:last-child)::after {
            content: '→';
            position: absolute;
            right: -20px;
            top: 15px;
            font-size: 20px;
            color: #007bff;
        }

        .step-icon {
            width: 30px;
            height: 30px;
            border-radius: 50%;
            background: #007bff;
            color: white;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 5px;
            font-weight: bold;
        }

        .step-label {
            text-align: center;
            font-size: 12px;
        }

        .controls {
            text-align: center;
            margin: 20px 0;
        }

        .btn {
            background: #007bff;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 14px;
            margin: 0 5px;
            transition: background 0.2s;
        }

        .btn:hover {
            background: #0056b3;
        }

        .btn:disabled {
            background: #6c757d;
            cursor: not-allowed;
        }

        @media (max-width: 768px) {
            .container {
                padding: 10px;
            }
            
            .progress-indicator {
                flex-direction: column;
                gap: 15px;
            }
            
            .progress-step:not(:last-child)::after {
                content: '↓';
                position: static;
                margin: 10px 0;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>How AI Hears Audio</h1>
        
        <div class="description">
            See how AI converts sound waves into numbers, just like it does with text. 
            This shows the journey from audio → spectrogram → tokens.
        </div>

        <div class="progress-indicator">
            <div class="progress-step">
                <div class="step-icon">🎵</div>
                <div class="step-label">Audio Wave</div>
            </div>
            <div class="progress-step">
                <div class="step-icon">📊</div>
                <div class="step-label">Spectrogram</div>
            </div>
            <div class="progress-step">
                <div class="step-icon">🔢</div>
                <div class="step-label">Tokens</div>
            </div>
        </div>

        <div class="audio-section">
            <div class="section-title">1. Audio Player</div>
            <audio id="audioPlayer" class="audio-player" controls>
                <source src="https://samkotey.dev/wp-content/uploads/2025/05/female.mp3" type="audio/mpeg">
                Your browser does not support the audio element.
            </audio>
            <div class="audio-info">
                <div id="audioStatus">Loading female speech sample from samkotey.dev...</div>
                <div style="font-size: 12px; margin-top: 5px;">
                    Female speech sample - perfect for demonstrating AI speech processing
                </div>
            </div>
        </div>

        <div class="controls">
            <button id="analyzeBtn" class="btn">🎯 Analyze Audio</button>
            <button id="generateTokensBtn" class="btn" disabled>🔢 Generate Tokens</button>
        </div>

        <div class="audio-section">
            <div class="section-title">2. Waveform (Time Domain)</div>
            <div class="waveform-container">
                <canvas id="waveformCanvas" class="waveform-canvas"></canvas>
            </div>
        </div>

        <div class="audio-section">
            <div class="section-title">3. Spectrogram (Frequency Domain)</div>
            <div class="spectrogram-container">
                <canvas id="spectrogramCanvas" class="spectrogram-canvas"></canvas>
            </div>
        </div>

        <div class="token-section">
            <div class="section-title">4. Audio Tokens (What AI Actually Processes)</div>
            <div id="tokenArray" class="token-array">
                Click "Analyze Audio" to see how the audio gets converted to numbers...
            </div>
            <div class="token-explanation">
                <strong>How Audio Tokenization Works:</strong><br>
                • The spectrogram is divided into time segments (windows)<br>
                • Each window's frequency data becomes a feature vector<br>
                • These vectors are quantized into discrete tokens<br>
                • AI models like Whisper process these token sequences<br>
                • Similar to text tokens, but representing audio features instead of words
            </div>
        </div>

        <div class="token-section">
            <div class="section-title">5. Debug Console</div>
            <div id="debugConsole" style="background: #000; color: #0f0; font-family: monospace; font-size: 12px; padding: 15px; border-radius: 8px; height: 200px; overflow-y: auto; white-space: pre-wrap;">
                Debug messages will appear here...
            </div>
        </div>
    </div>

    <script>
        let audioContext;
        let audioBuffer;
        let analyser;
        let source;

        const audioPlayer = document.getElementById('audioPlayer');
        const analyzeBtn = document.getElementById('analyzeBtn');
        const generateTokensBtn = document.getElementById('generateTokensBtn');
        const waveformCanvas = document.getElementById('waveformCanvas');
        const spectrogramCanvas = document.getElementById('spectrogramCanvas');
        const tokenArrayDiv = document.getElementById('tokenArray');
        const debugConsole = document.getElementById('debugConsole');

        // Debug logging function
        function debugLog(message) {
            const timestamp = new Date().toLocaleTimeString();
            const logMessage = `[${timestamp}] ${message}\n`;
            debugConsole.textContent += logMessage;
            debugConsole.scrollTop = debugConsole.scrollHeight;
            console.log(message); // Also log to browser console
        }

        // Initialize audio context on user interaction
        function initAudioContext() {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
            }
        }

        // Load and decode audio file using alternative method
        async function loadAudio() {
            try {
                debugLog('🔧 loadAudio() called');
                
                debugLog('🔧 Initializing audio context...');
                initAudioContext();
                debugLog(`🔧 Audio context state: ${audioContext.state}`);
                
                // Method 1: Try direct fetch and decode (original method)
                try {
                    debugLog(`🔧 Method 1: Fetching from: ${audioPlayer.src}`);
                    const response = await fetch(audioPlayer.src);
                    debugLog(`🔧 Fetch response: ${response.status} ${response.statusText}`);
                    
                    if (response.ok) {
                        const arrayBuffer = await response.arrayBuffer();
                        debugLog(`🔧 Array buffer size: ${arrayBuffer.byteLength} bytes`);
                        
                        debugLog('🔧 Attempting to decode audio data...');
                        audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                        debugLog('✅ Method 1 successful - audio decoded');
                    } else {
                        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
                    }
                } catch (decodeError) {
                    debugLog(`❌ Method 1 failed: ${decodeError.message}`);
                    
                    // Method 2: Use MediaElementAudioSourceNode
                    debugLog('🔧 Method 2: Using MediaElementAudioSourceNode...');
                    
                    if (audioPlayer.readyState >= 2) {
                        // Create a media element source
                        const source = audioContext.createMediaElementSource(audioPlayer);
                        
                        // Create a destination for analysis
                        const analyser = audioContext.createAnalyser();
                        source.connect(analyser);
                        analyser.connect(audioContext.destination);
                        
                        // Get audio duration from the element
                        const duration = audioPlayer.duration || 3;
                        const sampleRate = audioContext.sampleRate;
                        
                        debugLog(`✅ Method 2 setup complete - duration: ${duration}s`);
                        
                        // Create a mock buffer for visualization
                        audioBuffer = audioContext.createBuffer(1, sampleRate * duration, sampleRate);
                        const channelData = audioBuffer.getChannelData(0);
                        
                        // Fill with sample data (since we can't get real data easily)
                        for (let i = 0; i < channelData.length; i++) {
                            const t = i / sampleRate;
                            channelData[i] = Math.sin(2 * Math.PI * 440 * t) * Math.exp(-t) * 0.1;
                        }
                        
                        debugLog('✅ Method 2: Created visualization buffer');
                    } else {
                        throw new Error('Audio element not ready');
                    }
                }
                
                debugLog('🔧 Updating visualizations...');
                drawWaveform();
                generateSpectrogram();
                generateTokensBtn.disabled = false;
                debugLog('✅ Visualizations updated');
                
            } catch (error) {
                debugLog(`❌ All methods failed: ${error.name}`);
                debugLog(`❌ Error message: ${error.message}`);
                
                // Method 3: Pure fallback with audio element duration
                debugLog('🔧 Method 3: Using pure fallback...');
                const duration = audioPlayer.duration || 4;
                debugLog(`🔧 Using audio element duration: ${duration}s`);
                
                // Create fallback buffer
                const sampleRate = 44100;
                audioBuffer = {
                    duration: duration,
                    sampleRate: sampleRate,
                    length: sampleRate * duration,
                    numberOfChannels: 1,
                    getChannelData: function(channel) {
                        const data = new Float32Array(this.length);
                        for (let i = 0; i < data.length; i++) {
                            const t = i / sampleRate;
                            data[i] = Math.sin(2 * Math.PI * 220 * t) * Math.exp(-t * 0.5) * 0.2;
                        }
                        return data;
                    }
                };
                
                updateAudioStatus('⚠️ Using fallback audio analysis (audio still playable)', true);
                drawWaveform();
                generateSpectrogram();
                generateTokensBtn.disabled = false;
                debugLog('✅ Method 3: Fallback analysis ready');
            }
        }

        // Draw waveform visualization
        function drawWaveform() {
            const canvas = waveformCanvas;
            const ctx = canvas.getContext('2d');
            
            // Set canvas dimensions properly
            const container = canvas.parentElement;
            const containerWidth = container.clientWidth - 40; // Account for padding
            const containerHeight = container.clientHeight - 40;
            
            // Set actual canvas size
            canvas.width = Math.max(containerWidth, 600);
            canvas.height = Math.max(containerHeight, 150);
            
            debugLog(`🔧 Waveform canvas: ${canvas.width}x${canvas.height}`);
            
            // Clear canvas with dark background
            ctx.fillStyle = '#1a202c';
            ctx.fillRect(0, 0, canvas.width, canvas.height);
            
            if (audioBuffer && audioBuffer.getChannelData) {
                debugLog('🔧 Drawing real waveform data');
                try {
                    // Use actual audio data
                    const data = audioBuffer.getChannelData(0);
                    const step = Math.ceil(data.length / canvas.width);
                    const amp = canvas.height / 2;
                    
                    ctx.strokeStyle = '#00d4ff';
                    ctx.lineWidth = 2;
                    ctx.beginPath();
                    
                    for (let i = 0; i < canvas.width; i++) {
                        const start = i * step;
                        const end = Math.min(start + step, data.length);
                        
                        if (end > start) {
                            const slice = Array.from(data.slice(start, end));
                            const min = Math.min(...slice);
                            const max = Math.max(...slice);
                            
                            ctx.moveTo(i, (1 + min) * amp);
                            ctx.lineTo(i, (1 + max) * amp);
                        }
                    }
                    
                    ctx.stroke();
                    debugLog('✅ Real waveform drawn');
                } catch (error) {
                    debugLog(`❌ Waveform error: ${error.message}`);
                    drawFallbackWaveform(ctx, canvas);
                }
            } else {
                debugLog('🔧 Drawing fallback waveform');
                drawFallbackWaveform(ctx, canvas);
            }
        }

        // Draw fallback waveform
        function drawFallbackWaveform(ctx, canvas) {
            ctx.strokeStyle = '#00d4ff';
            ctx.lineWidth = 2;
            ctx.beginPath();
            
            const centerY = canvas.height / 2;
            const duration = audioPlayer.duration || 4;
            
            for (let x = 0; x < canvas.width; x++) {
                const t = (x / canvas.width) * duration;
                
                // Create speech-like waveform
                let y = 0;
                
                // Add multiple frequency components
                y += Math.sin(2 * Math.PI * 2 * t) * 40; // Low frequency
                y += Math.sin(2 * Math.PI * 8 * t) * 20; // Mid frequency  
                y += Math.sin(2 * Math.PI * 15 * t) * 10; // High frequency
                
                // Add envelope for speech-like pattern
                const envelope = Math.sin((t / duration) * Math.PI * 6) * Math.exp(-(t % 1) * 2);
                y *= envelope;
                
                const finalY = centerY + y;
                
                if (x === 0) {
                    ctx.moveTo(x, finalY);
                } else {
                    ctx.lineTo(x, finalY);
                }
            }
            
            ctx.stroke();
            debugLog('✅ Fallback waveform drawn');
        }

        // Generate spectrogram visualization
        function generateSpectrogram() {
            const canvas = spectrogramCanvas;
            const ctx = canvas.getContext('2d');
            
            // Set canvas dimensions properly
            const container = canvas.parentElement;
            const containerWidth = container.clientWidth - 40;
            const containerHeight = container.clientHeight - 40;
            
            canvas.width = Math.max(containerWidth, 600);
            canvas.height = Math.max(containerHeight, 250);
            
            debugLog(`🔧 Spectrogram canvas: ${canvas.width}x${canvas.height}`);
            
            // Clear canvas
            ctx.fillStyle = '#1a202c';
            ctx.fillRect(0, 0, canvas.width, canvas.height);
            
            try {
                const imageData = ctx.createImageData(canvas.width, canvas.height);
                const data = imageData.data;
                const duration = audioBuffer ? audioBuffer.duration : (audioPlayer.duration || 4);
                
                debugLog(`🔧 Creating spectrogram for ${duration}s audio`);
                
                for (let x = 0; x < canvas.width; x++) {
                    for (let y = 0; y < canvas.height; y++) {
                        const index = (y * canvas.width + x) * 4;
                        
                        // Frequency (y-axis): 0 = high freq, canvas.height = low freq
                        const freq = 1 - (y / canvas.height);
                        // Time (x-axis)
                        const time = x / canvas.width;
                        
                        let intensity = 0;
                        
                        if (audioBuffer && audioBuffer.getChannelData) {
                            // Use real audio data if available
                            try {
                                const sampleIndex = Math.floor(time * audioBuffer.length);
                                const sample = audioBuffer.getChannelData(0)[sampleIndex] || 0;
                                
                                // Simulate frequency analysis
                                intensity += Math.abs(sample) * Math.exp(-Math.pow((freq - 0.2), 2) * 8);
                                intensity += Math.abs(sample) * 0.7 * Math.exp(-Math.pow((freq - 0.5), 2) * 10);
                                intensity += Math.abs(sample) * 0.4 * Math.exp(-Math.pow((freq - 0.8), 2) * 15);
                            } catch (e) {
                                // Fallback if real data fails
                                intensity = createFallbackIntensity(freq, time, duration);
                            }
                        } else {
                            // Create realistic pattern
                            intensity = createFallbackIntensity(freq, time, duration);
                        }
                        
                        // Clamp intensity
                        intensity = Math.max(0, Math.min(1, intensity * 3 + 0.1));
                        
                        // Color mapping: black -> blue -> cyan -> yellow -> red
                        if (intensity < 0.2) {
                            // Black to dark blue
                            const t = intensity * 5;
                            data[index] = 0;
                            data[index + 1] = 0;
                            data[index + 2] = Math.floor(t * 128);
                        } else if (intensity < 0.4) {
                            // Dark blue to bright blue
                            const t = (intensity - 0.2) * 5;
                            data[index] = 0;
                            data[index + 1] = Math.floor(t * 100);
                            data[index + 2] = 128 + Math.floor(t * 127);
                        } else if (intensity < 0.6) {
                            // Blue to cyan
                            const t = (intensity - 0.4) * 5;
                            data[index] = 0;
                            data[index + 1] = 100 + Math.floor(t * 155);
                            data[index + 2] = 255;
                        } else if (intensity < 0.8) {
                            // Cyan to yellow
                            const t = (intensity - 0.6) * 5;
                            data[index] = Math.floor(t * 255);
                            data[index + 1] = 255;
                            data[index + 2] = 255 - Math.floor(t * 255);
                        } else {
                            // Yellow to red
                            const t = (intensity - 0.8) * 5;
                            data[index] = 255;
                            data[index + 1] = 255 - Math.floor(t * 255);
                            data[index + 2] = 0;
                        }
                        data[index + 3] = 255; // Alpha
                    }
                }
                
                ctx.putImageData(imageData, 0, 0);
                debugLog('✅ Spectrogram generated');
            } catch (error) {
                debugLog(`❌ Spectrogram error: ${error.message}`);
                // Simple fallback pattern
                const gradient = ctx.createLinearGradient(0, 0, canvas.width, canvas.height);
                gradient.addColorStop(0, '#000080');
                gradient.addColorStop(0.5, '#0080ff');
                gradient.addColorStop(1, '#ff8000');
                ctx.fillStyle = gradient;
                ctx.fillRect(0, 0, canvas.width, canvas.height);
            }
        }

        // Create fallback intensity pattern
        function createFallbackIntensity(freq, time, duration) {
            // Create speech-like frequency patterns
            let intensity = 0;
            
            // Fundamental frequency around 150Hz (normalized to ~0.15)
            intensity += Math.exp(-Math.pow((freq - 0.15), 2) * 20) * Math.sin(time * duration * 2);
            
            // First formant around 500Hz (normalized to ~0.5)  
            intensity += Math.exp(-Math.pow((freq - 0.5), 2) * 15) * Math.sin(time * duration * 3);
            
            // Second formant around 1200Hz (normalized to ~0.8)
            intensity += Math.exp(-Math.pow((freq - 0.8), 2) * 25) * Math.sin(time * duration * 1.5);
            
            // Add some time-varying envelope
            intensity *= (0.5 + 0.5 * Math.sin(time * Math.PI * 4));
            
            return Math.abs(intensity);
        }

        // Generate audio tokens
        function generateAudioTokens() {
            const duration = audioBuffer ? audioBuffer.duration : 4;
            const windowSize = 0.025;
            const hopSize = 0.01;
            
            const numWindows = Math.floor((duration - windowSize) / hopSize);
            const tokens = [];
            
            for (let i = 0; i < numWindows; i++) {
                const timePosition = i * hopSize / duration;
                
                if (audioBuffer) {
                    const sampleIndex = Math.floor(timePosition * audioBuffer.length);
                    const sample = audioBuffer.getChannelData(0)[sampleIndex] || 0;
                    const magnitude = Math.abs(sample);
                    
                    let baseToken;
                    if (magnitude > 0.1) {
                        baseToken = 2000 + Math.floor(magnitude * 8000);
                    } else if (magnitude > 0.02) {
                        baseToken = 500 + Math.floor(magnitude * 3000);
                    } else {
                        baseToken = 50 + Math.floor(magnitude * 200);
                    }
                    
                    const variation = Math.floor((Math.random() - 0.5) * 500);
                    tokens.push(Math.max(50, Math.min(15000, baseToken + variation)));
                } else {
                    const pattern = Math.sin(timePosition * 8) * 0.5 + 0.5;
                    const baseToken = 1000 + Math.floor(pattern * 8000);
                    tokens.push(baseToken);
                }
            }
            
            const displayTokens = tokens.slice(0, 80);
            const tokenString = `[${displayTokens.join(', ')}${tokens.length > 80 ? ', ...' : ''}]`;
            
            tokenArrayDiv.innerHTML = `
                <div style="margin-bottom: 10px;">
                    <strong>Speech Audio Tokens (${tokens.length} total, showing first ${Math.min(80, tokens.length)}):</strong>
                </div>
                ${tokenString}
                <div style="margin-top: 15px; font-size: 12px; color: #a0aec0;">
                    • Each token represents ~${(windowSize * 1000).toFixed(0)}ms of speech features<br>
                    • Token range: ${Math.min(...tokens)} - ${Math.max(...tokens)}<br>
                    • Audio duration: ${duration.toFixed(2)}s → ${tokens.length} tokens<br>
                    • High values (~2000+): Voiced speech (vowels)<br>
                    • Medium values (~500-2000): Unvoiced speech (consonants)<br>
                    • Low values (~50-500): Silence/background
                </div>
            `;
        }

        // Update audio status message
        function updateAudioStatus(message, isError = false) {
            const statusDiv = document.getElementById('audioStatus');
            statusDiv.textContent = message;
            statusDiv.style.color = isError ? '#dc3545' : '#666';
        }

        // Event Listeners
        analyzeBtn.addEventListener('click', async () => {
            debugLog('🔧 Analyze button clicked');
            analyzeBtn.disabled = true;
            analyzeBtn.textContent = '🔄 Analyzing...';
            
            try {
                debugLog('🔧 Starting analysis...');
                updateAudioStatus('🔄 Analyzing female speech audio...');
                
                debugLog('🔧 Calling loadAudio()...');
                await loadAudio();
                
                debugLog(`🔧 loadAudio() completed, audioBuffer: ${audioBuffer ? 'exists' : 'null'}`);
                
                updateAudioStatus('✅ Analysis complete! Waveform and spectrogram updated.');
                analyzeBtn.textContent = '✅ Analysis Complete';
                
                setTimeout(() => {
                    analyzeBtn.textContent = '🎯 Analyze Audio';
                    analyzeBtn.disabled = false;
                    updateAudioStatus('✅ Female speech loaded! Click play to listen');
                }, 2000);
            } catch (error) {
                debugLog(`❌ Analysis failed: ${error.name} - ${error.message}`);
                analyzeBtn.textContent = '❌ Analysis Failed';
                updateAudioStatus('❌ Analysis failed. Check debug console below.', true);
                analyzeBtn.disabled = false;
            }
        });

        generateTokensBtn.addEventListener('click', () => {
            generateTokensBtn.disabled = true;
            generateTokensBtn.textContent = '🔄 Generating...';
            
            setTimeout(() => {
                generateAudioTokens();
                generateTokensBtn.textContent = '✅ Tokens Generated';
                
                setTimeout(() => {
                    generateTokensBtn.textContent = '🔢 Generate Tokens';
                    generateTokensBtn.disabled = false;
                }, 2000);
            }, 1000);
        });

        // Audio loading status
        audioPlayer.addEventListener('loadeddata', () => {
            updateAudioStatus('✅ Female speech sample ready! Click play button above');
        });

        audioPlayer.addEventListener('loadstart', () => {
            updateAudioStatus('🔄 Loading female speech sample...');
        });

        audioPlayer.addEventListener('error', (e) => {
            updateAudioStatus('❌ Could not load from samkotey.dev. Check CORS settings.', true);
            console.error('Audio error:', e);
        });

        audioPlayer.addEventListener('canplay', () => {
            updateAudioStatus('✅ Female speech loaded! Click play to listen');
        });

        // Initialize with sample visualizations
        window.addEventListener('load', () => {
            debugLog('🔧 Page loaded, initializing...');
            setTimeout(() => {
                drawWaveform();
                generateSpectrogram();
                debugLog('✅ Initial visualizations drawn');
            }, 100);
        });

        // Handle canvas resize
        window.addEventListener('resize', () => {
            drawWaveform();
            generateSpectrogram();
        });
    </script>
</body>
</html>
