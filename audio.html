<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How AI Hears Audio</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Helvetica', 'Arial', sans-serif;
            background: #ffffff;
            margin: 0;
            padding: 20px;
            color: #333;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            background: #ffffff;
        }

        h1 {
            text-align: center;
            margin-bottom: 30px;
            font-size: 2em;
            color: #333;
        }

        .description {
            text-align: center;
            color: #666;
            margin-bottom: 30px;
            font-size: 16px;
            line-height: 1.5;
        }

        .audio-section {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
        }

        .section-title {
            font-weight: bold;
            margin-bottom: 15px;
            color: #333;
            font-size: 18px;
        }

        .audio-player {
            width: 100%;
            margin-bottom: 15px;
        }

        .audio-info {
            font-size: 14px;
            color: #666;
            text-align: center;
        }

        .waveform-container {
            background: #2d3748;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            height: 200px;
            position: relative;
            overflow: hidden;
        }

        .waveform-canvas {
            width: 100%;
            height: 100%;
            background: #1a202c;
            border-radius: 4px;
        }

        .spectrogram-container {
            background: #2d3748;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            height: 300px;
            position: relative;
            overflow: hidden;
        }

        .spectrogram-canvas {
            width: 100%;
            height: 100%;
            background: #1a202c;
            border-radius: 4px;
        }

        .token-section {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
        }

        .token-array {
            background: #2d3748;
            color: #e2e8f0;
            border: 1px solid #4a5568;
            border-radius: 8px;
            padding: 15px;
            font-family: 'Monaco', 'Menlo', 'Consolas', monospace;
            font-size: 13px;
            line-height: 1.4;
            overflow-x: auto;
            white-space: pre-wrap;
            word-break: break-all;
            margin-bottom: 15px;
        }

        .token-explanation {
            font-size: 14px;
            color: #666;
            background: #f0f8ff;
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #007bff;
        }

        .progress-indicator {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 20px 0;
            font-size: 14px;
            color: #666;
        }

        .progress-step {
            display: flex;
            flex-direction: column;
            align-items: center;
            flex: 1;
            position: relative;
        }

        .progress-step:not(:last-child)::after {
            content: '‚Üí';
            position: absolute;
            right: -20px;
            top: 15px;
            font-size: 20px;
            color: #007bff;
        }

        .step-icon {
            width: 30px;
            height: 30px;
            border-radius: 50%;
            background: #007bff;
            color: white;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 5px;
            font-weight: bold;
        }

        .step-label {
            text-align: center;
            font-size: 12px;
        }

        .controls {
            text-align: center;
            margin: 20px 0;
        }

        .btn {
            background: #007bff;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 14px;
            margin: 0 5px;
            transition: background 0.2s;
        }

        .btn:hover {
            background: #0056b3;
        }

        .btn:disabled {
            background: #6c757d;
            cursor: not-allowed;
        }

        .demo-mode {
            background: #e8f4fd;
            border: 2px solid #007bff;
            border-radius: 8px;
            padding: 15px;
            margin: 15px 0;
            text-align: center;
        }

        @media (max-width: 768px) {
            .container {
                padding: 10px;
            }
            
            .progress-indicator {
                flex-direction: column;
                gap: 15px;
            }
            
            .progress-step:not(:last-child)::after {
                content: '‚Üì';
                position: static;
                margin: 10px 0;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>How AI Hears Audio</h1>
        
        <div class="description">
            See how AI converts sound waves into numbers, just like it does with text. 
            This demonstrates the journey from audio ‚Üí spectrogram ‚Üí tokens.
        </div>

        <div class="progress-indicator">
            <div class="progress-step">
                <div class="step-icon">üéµ</div>
                <div class="step-label">Audio Wave</div>
            </div>
            <div class="progress-step">
                <div class="step-icon">üìä</div>
                <div class="step-label">Spectrogram</div>
            </div>
            <div class="progress-step">
                <div class="step-icon">üî¢</div>
                <div class="step-label">Tokens</div>
            </div>
        </div>

        <div class="audio-section">
            <div class="section-title">1. Audio Player</div>
            <audio id="audioPlayer" class="audio-player" controls>
                <source src="https://www.w3schools.com/html/horse.mp3" type="audio/mpeg">
                Your browser does not support the audio element.
            </audio>
            <div class="demo-mode">
                <strong>üé¨ Demo Mode Active</strong><br>
                Live visualization running below to show how AI processes audio
            </div>
        </div>

        <div class="controls">
            <button id="startDemo" class="btn">üé¨ Start Live Demo</button>
            <button id="stopDemo" class="btn">‚èπÔ∏è Stop Demo</button>
            <button id="generateTokens" class="btn">üî¢ Generate Tokens</button>
        </div>

        <div class="audio-section">
            <div class="section-title">2. Waveform (Time Domain)</div>
            <div class="waveform-container">
                <canvas id="waveformCanvas" class="waveform-canvas"></canvas>
            </div>
        </div>

        <div class="audio-section">
            <div class="section-title">3. Spectrogram (Frequency Domain)</div>
            <div class="spectrogram-container">
                <canvas id="spectrogramCanvas" class="spectrogram-canvas"></canvas>
            </div>
        </div>

        <div class="token-section">
            <div class="section-title">4. Audio Tokens (What AI Actually Processes)</div>
            <div id="tokenArray" class="token-array">
                Click "Generate Tokens" to see how audio gets converted to numbers...
            </div>
            <div class="token-explanation">
                <strong>How Audio Tokenization Works:</strong><br>
                ‚Ä¢ The spectrogram is divided into time segments (windows)<br>
                ‚Ä¢ Each window's frequency data becomes a feature vector<br>
                ‚Ä¢ These vectors are quantized into discrete tokens<br>
                ‚Ä¢ AI models like Whisper process these token sequences<br>
                ‚Ä¢ Similar to text tokens, but representing audio features instead of words
            </div>
        </div>
    </div>

    <script>
        let isRunning = false;
        let animationId = null;
        let frame = 0;

        const waveCanvas = document.getElementById('waveformCanvas');
        const specCanvas = document.getElementById('spectrogramCanvas');
        const startBtn = document.getElementById('startDemo');
        const stopBtn = document.getElementById('stopDemo');
        const tokenBtn = document.getElementById('generateTokens');
        const tokenDiv = document.getElementById('tokenArray');

        // Initialize canvases
        function initCanvases() {
            // Waveform canvas
            waveCanvas.width = waveCanvas.parentElement.clientWidth - 40;
            waveCanvas.height = waveCanvas.parentElement.clientHeight - 40;
            
            // Spectrogram canvas
            specCanvas.width = specCanvas.parentElement.clientWidth - 40;
            specCanvas.height = specCanvas.parentElement.clientHeight - 40;
        }

        // Draw animated waveform
        function drawWaveform() {
            const ctx = waveCanvas.getContext('2d');
            const width = waveCanvas.width;
            const height = waveCanvas.height;
            
            // Clear
            ctx.fillStyle = '#1a202c';
            ctx.fillRect(0, 0, width, height);
            
            // Draw waveform
            ctx.strokeStyle = '#00d4ff';
            ctx.lineWidth = 2;
            ctx.beginPath();
            
            const centerY = height / 2;
            
            for (let x = 0; x < width; x++) {
                const t = (x / width) * 4; // 4 seconds of "audio"
                const time = t + frame * 0.02; // Moving time
                
                // Create speech-like waveform
                let y = 0;
                y += Math.sin(time * Math.PI * 2 * 150) * 50; // 150Hz fundamental
                y += Math.sin(time * Math.PI * 2 * 300) * 25; // 300Hz harmonic
                y += Math.sin(time * Math.PI * 2 * 450) * 15; // 450Hz harmonic
                
                // Add speech envelope (words and pauses)
                const wordCycle = (time * 2) % 2; // 2-second cycle
                let envelope = 0;
                if (wordCycle < 1.2) { // 1.2s speech, 0.8s pause
                    envelope = Math.sin(wordCycle * Math.PI / 1.2);
                }
                
                y *= envelope;
                
                // Add some noise
                y += (Math.random() - 0.5) * 10;
                
                const finalY = centerY + y;
                
                if (x === 0) {
                    ctx.moveTo(x, finalY);
                } else {
                    ctx.lineTo(x, finalY);
                }
            }
            
            ctx.stroke();
        }

        // Draw scrolling spectrogram
        function drawSpectrogram() {
            const ctx = specCanvas.getContext('2d');
            const width = specCanvas.width;
            const height = specCanvas.height;
            
            // Shift existing content left
            if (width > 1) {
                const imageData = ctx.getImageData(1, 0, width - 1, height);
                ctx.putImageData(imageData, 0, 0);
            }
            
            // Draw new column on the right
            const x = width - 1;
            const time = frame * 0.02;
            
            for (let y = 0; y < height; y++) {
                // Map y to frequency (bottom = low freq, top = high freq)
                const freq = (height - y) / height;
                
                // Create speech-like frequency content
                let intensity = 0;
                
                // Fundamental frequency around 150Hz (normalized to ~0.15)
                intensity += Math.exp(-Math.pow((freq - 0.15), 2) * 30) * 
                           (Math.sin(time * 4) * 0.5 + 0.5) * 0.8;
                
                // First formant around 800Hz (normalized to ~0.4)
                intensity += Math.exp(-Math.pow((freq - 0.4), 2) * 20) * 
                           (Math.sin(time * 3 + 1) * 0.5 + 0.5) * 0.6;
                
                // Second formant around 1200Hz (normalized to ~0.6)
                intensity += Math.exp(-Math.pow((freq - 0.6), 2) * 25) * 
                           (Math.sin(time * 2 + 2) * 0.5 + 0.5) * 0.4;
                
                // Higher frequencies with less energy
                intensity += Math.exp(-Math.pow((freq - 0.8), 2) * 40) * 
                           (Math.sin(time * 5 + 3) * 0.5 + 0.5) * 0.2;
                
                // Add speech envelope
                const wordCycle = (time * 2) % 2;
                let envelope = 0;
                if (wordCycle < 1.2) {
                    envelope = Math.sin(wordCycle * Math.PI / 1.2);
                }
                
                intensity *= envelope;
                intensity = Math.max(0, Math.min(1, intensity));
                
                // Color mapping
                let r, g, b;
                if (intensity < 0.2) {
                    // Black to blue
                    r = 0; g = 0; b = Math.floor(intensity * 5 * 255);
                } else if (intensity < 0.4) {
                    // Blue to cyan
                    const t = (intensity - 0.2) / 0.2;
                    r = 0; g = Math.floor(t * 255); b = 255;
                } else if (intensity < 0.7) {
                    // Cyan to yellow
                    const t = (intensity - 0.4) / 0.3;
                    r = Math.floor(t * 255); g = 255; b = 255 - Math.floor(t * 255);
                } else {
                    // Yellow to red
                    const t = (intensity - 0.7) / 0.3;
                    r = 255; g = 255 - Math.floor(t * 255); b = 0;
                }
                
                ctx.fillStyle = `rgb(${r},${g},${b})`;
                ctx.fillRect(x, y, 1, 1);
            }
        }

        // Animation loop
        function animate() {
            if (!isRunning) return;
            
            frame++;
            drawWaveform();
            drawSpectrogram();
            
            animationId = requestAnimationFrame(animate);
        }

        // Start demo
        function startDemo() {
            if (isRunning) return;
            
            isRunning = true;
            startBtn.disabled = true;
            stopBtn.disabled = false;
            
            initCanvases();
            animate();
        }

        // Stop demo
        function stopDemo() {
            isRunning = false;
            startBtn.disabled = false;
            stopBtn.disabled = true;
            
            if (animationId) {
                cancelAnimationFrame(animationId);
            }
        }

        // Generate tokens
        function generateTokens() {
            const duration = 4; // 4 seconds of simulated audio
            const windowSize = 0.025; // 25ms windows
            const hopSize = 0.01; // 10ms hop
            const numWindows = Math.floor((duration - windowSize) / hopSize);
            const tokens = [];
            
            for (let i = 0; i < numWindows; i++) {
                const timePos = i * hopSize / duration;
                
                // Simulate speech tokens
                const wordCycle = (timePos * 2) % 2;
                let baseToken;
                
                if (wordCycle < 1.2) {
                    // Speech segment
                    baseToken = 2000 + Math.floor(Math.random() * 6000);
                } else {
                    // Silence segment
                    baseToken = 100 + Math.floor(Math.random() * 300);
                }
                
                tokens.push(baseToken);
            }
            
            const tokenString = `[${tokens.slice(0, 50).join(', ')}${tokens.length > 50 ? ', ...' : ''}]`;
            
            tokenDiv.innerHTML = `
                <div style="margin-bottom: 10px;">
                    <strong>Speech Audio Tokens (${tokens.length} total):</strong>
                </div>
                ${tokenString}
                <div style="margin-top: 15px; font-size: 12px; color: #a0aec0;">
                    ‚Ä¢ Duration: ${duration}s ‚Üí ${tokens.length} tokens<br>
                    ‚Ä¢ High values (2000+): Speech segments<br>
                    ‚Ä¢ Low values (100-400): Silence segments<br>
                    ‚Ä¢ Each token represents ~25ms of audio
                </div>
            `;
        }

        // Event listeners
        startBtn.addEventListener('click', startDemo);
        stopBtn.addEventListener('click', stopDemo);
        tokenBtn.addEventListener('click', generateTokens);

        // Auto-start after page load
        window.addEventListener('load', () => {
            setTimeout(startDemo, 1000);
        });

        // Handle resize
        window.addEventListener('resize', () => {
            if (isRunning) {
                initCanvases();
            }
        });
    </script>
</body>
</html>
