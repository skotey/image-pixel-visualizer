<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How AI Hears Audio</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Helvetica', 'Arial', sans-serif;
            background: #ffffff;
            margin: 0;
            padding: 20px;
            color: #333;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            background: #ffffff;
        }

        h1 {
            text-align: center;
            margin-bottom: 30px;
            font-size: 2em;
            color: #333;
        }

        .description {
            text-align: center;
            color: #666;
            margin-bottom: 30px;
            font-size: 16px;
            line-height: 1.5;
        }

        .audio-section {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
        }

        .section-title {
            font-weight: bold;
            margin-bottom: 15px;
            color: #333;
            font-size: 18px;
        }

        .audio-player {
            width: 100%;
            margin-bottom: 15px;
        }

        .audio-info {
            font-size: 14px;
            color: #666;
            text-align: center;
        }

        .waveform-container {
            background: #2d3748;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            height: 200px;
            position: relative;
            overflow: hidden;
        }

        .waveform-canvas {
            width: 100%;
            height: 100%;
            background: #1a202c;
            border-radius: 4px;
        }

        .spectrogram-container {
            background: #2d3748;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            height: 300px;
            position: relative;
            overflow: hidden;
        }

        .spectrogram-canvas {
            width: 100%;
            height: 100%;
            background: #1a202c;
            border-radius: 4px;
        }

        .token-section {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
        }

        .token-array {
            background: #2d3748;
            color: #e2e8f0;
            border: 1px solid #4a5568;
            border-radius: 8px;
            padding: 15px;
            font-family: 'Monaco', 'Menlo', 'Consolas', monospace;
            font-size: 13px;
            line-height: 1.4;
            overflow-x: auto;
            white-space: pre-wrap;
            word-break: break-all;
            margin-bottom: 15px;
        }

        .token-explanation {
            font-size: 14px;
            color: #666;
            background: #f0f8ff;
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #007bff;
        }

        .progress-indicator {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 20px 0;
            font-size: 14px;
            color: #666;
        }

        .progress-step {
            display: flex;
            flex-direction: column;
            align-items: center;
            flex: 1;
            position: relative;
        }

        .progress-step:not(:last-child)::after {
            content: '→';
            position: absolute;
            right: -20px;
            top: 15px;
            font-size: 20px;
            color: #007bff;
        }

        .step-icon {
            width: 30px;
            height: 30px;
            border-radius: 50%;
            background: #007bff;
            color: white;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 5px;
            font-weight: bold;
        }

        .step-label {
            text-align: center;
            font-size: 12px;
        }

        .controls {
            text-align: center;
            margin: 20px 0;
        }

        .btn {
            background: #007bff;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 14px;
            margin: 0 5px;
            transition: background 0.2s;
        }

        .btn:hover {
            background: #0056b3;
        }

        .btn:disabled {
            background: #6c757d;
            cursor: not-allowed;
        }

        @media (max-width: 768px) {
            .container {
                padding: 10px;
            }
            
            .progress-indicator {
                flex-direction: column;
                gap: 15px;
            }
            
            .progress-step:not(:last-child)::after {
                content: '↓';
                position: static;
                margin: 10px 0;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>How AI Hears Audio</h1>
        
        <div class="description">
            See how AI converts sound waves into numbers, just like it does with text. 
            This shows the journey from audio → spectrogram → tokens.
        </div>

        <div class="progress-indicator">
            <div class="progress-step">
                <div class="step-icon">🎵</div>
                <div class="step-label">Audio Wave</div>
            </div>
            <div class="progress-step">
                <div class="step-icon">📊</div>
                <div class="step-label">Spectrogram</div>
            </div>
            <div class="progress-step">
                <div class="step-icon">🔢</div>
                <div class="step-label">Tokens</div>
            </div>
        </div>

        <div class="audio-section">
            <div class="section-title">1. Audio Player</div>
            <audio id="audioPlayer" class="audio-player" controls>
                <source src="https://www.soundjay.com/misc/sounds/bell-ringing-05.mp3" type="audio/mpeg">
                Your browser does not support the audio element.
            </audio>
            <div class="audio-info">
                <div id="audioStatus">Loading female speech sample from samkotey.dev...</div>
                <div style="font-size: 12px; margin-top: 5px;">
                    Female speech sample - perfect for demonstrating AI speech processing
                </div>
            </div>
        </div>

        <div class="controls">
            <button id="analyzeBtn" class="btn">🎯 Analyze Audio</button>
            <button id="generateTokensBtn" class="btn" disabled>🔢 Generate Tokens</button>
        </div>

        <div class="audio-section">
            <div class="section-title">2. Waveform (Time Domain)</div>
            <div class="waveform-container">
                <canvas id="waveformCanvas" class="waveform-canvas"></canvas>
            </div>
        </div>

        <div class="audio-section">
            <div class="section-title">3. Spectrogram (Frequency Domain)</div>
            <div class="spectrogram-container">
                <canvas id="spectrogramCanvas" class="spectrogram-canvas"></canvas>
            </div>
        </div>

        <div class="token-section">
            <div class="section-title">4. Audio Tokens (What AI Actually Processes)</div>
            <div id="tokenArray" class="token-array">
                Click "Analyze Audio" to see how the audio gets converted to numbers...
            </div>
            <div class="token-explanation">
                <strong>How Audio Tokenization Works:</strong><br>
                • The spectrogram is divided into time segments (windows)<br>
                • Each window's frequency data becomes a feature vector<br>
                • These vectors are quantized into discrete tokens<br>
                • AI models like Whisper process these token sequences<br>
                • Similar to text tokens, but representing audio features instead of words
            </div>
        </div>

        <div class="token-section">
            <div class="section-title">5. Debug Console</div>
            <div id="debugConsole" style="background: #000; color: #0f0; font-family: monospace; font-size: 12px; padding: 15px; border-radius: 8px; height: 200px; overflow-y: auto; white-space: pre-wrap;">
                Debug messages will appear here...
            </div>
        </div>
    </div>

    <script>
        let audioContext;
        let audioBuffer;
        let analyser;
        let source;

        const audioPlayer = document.getElementById('audioPlayer');
        const analyzeBtn = document.getElementById('analyzeBtn');
        const generateTokensBtn = document.getElementById('generateTokensBtn');
        const waveformCanvas = document.getElementById('waveformCanvas');
        const spectrogramCanvas = document.getElementById('spectrogramCanvas');
        const tokenArrayDiv = document.getElementById('tokenArray');
        const debugConsole = document.getElementById('debugConsole');

        // Debug logging function
        function debugLog(message) {
            const timestamp = new Date().toLocaleTimeString();
            const logMessage = `[${timestamp}] ${message}\n`;
            debugConsole.textContent += logMessage;
            debugConsole.scrollTop = debugConsole.scrollHeight;
            console.log(message); // Also log to browser console
        }

        // Initialize audio context on user interaction
        function initAudioContext() {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
            }
        }

        // Load and decode audio file
        async function loadAudio() {
            try {
                debugLog('🔧 loadAudio() called');
                
                debugLog('🔧 Initializing audio context...');
                initAudioContext();
                debugLog(`🔧 Audio context state: ${audioContext.state}`);
                
                debugLog(`🔧 Audio player src: ${audioPlayer.src}`);
                debugLog('🔧 Fetching audio...');
                const response = await fetch(audioPlayer.src);
                
                debugLog(`🔧 Fetch response: ${response.status} ${response.statusText}`);
                if (!response.ok) {
                    throw new Error(`HTTP ${response.status}: ${response.statusText}`);
                }
                
                debugLog('🔧 Getting array buffer...');
                const arrayBuffer = await response.arrayBuffer();
                debugLog(`🔧 Array buffer size: ${arrayBuffer.byteLength} bytes`);
                
                debugLog('🔧 Decoding audio data...');
                audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                
                debugLog(`✅ Audio loaded successfully:`);
                debugLog(`   Duration: ${audioBuffer.duration}s`);
                debugLog(`   Sample rate: ${audioBuffer.sampleRate}Hz`);
                debugLog(`   Channels: ${audioBuffer.numberOfChannels}`);
                debugLog(`   Length: ${audioBuffer.length} samples`);
                
                debugLog('🔧 Updating visualizations...');
                // Update visualizations with real data
                drawWaveform();
                generateSpectrogram();
                generateTokensBtn.disabled = false;
                debugLog('✅ Visualizations updated');
                
            } catch (error) {
                debugLog(`❌ Error in loadAudio(): ${error.name}`);
                debugLog(`❌ Error message: ${error.message}`);
                if (error.stack) {
                    debugLog(`❌ Error stack: ${error.stack.substring(0, 200)}...`);
                }
                updateAudioStatus('⚠️ Could not analyze audio data. Using demo visualizations.', true);
                // Keep demo visualizations
                drawWaveform();
                generateSpectrogram();
                generateTokensBtn.disabled = false;
                throw error; // Re-throw so the button handler can catch it
            }
        }

        // Draw waveform visualization
        function drawWaveform() {
            const canvas = waveformCanvas;
            const ctx = canvas.getContext('2d');
            
            const containerWidth = canvas.parentElement.offsetWidth - 40;
            const containerHeight = canvas.parentElement.offsetHeight - 40;
            
            canvas.width = Math.max(containerWidth, 400);
            canvas.height = Math.max(containerHeight, 150);
            
            // Clear canvas
            ctx.fillStyle = '#1a202c';
            ctx.fillRect(0, 0, canvas.width, canvas.height);
            
            if (audioBuffer) {
                // Use actual audio data
                const data = audioBuffer.getChannelData(0);
                const step = Math.ceil(data.length / canvas.width);
                const amp = canvas.height / 2;
                
                ctx.strokeStyle = '#00d4ff';
                ctx.lineWidth = 1;
                ctx.beginPath();
                
                for (let i = 0; i < canvas.width; i++) {
                    const start = i * step;
                    const end = Math.min(start + step, data.length);
                    
                    if (end > start) {
                        const slice = data.slice(start, end);
                        const min = Math.min(...slice);
                        const max = Math.max(...slice);
                        
                        ctx.moveTo(i, (1 + min) * amp);
                        ctx.lineTo(i, (1 + max) * amp);
                    }
                }
                
                ctx.stroke();
            } else {
                // Generate sample waveform
                ctx.strokeStyle = '#00d4ff';
                ctx.lineWidth = 2;
                ctx.beginPath();
                
                const centerY = canvas.height / 2;
                
                for (let x = 0; x < canvas.width; x++) {
                    const t = x / canvas.width;
                    let y = 0;
                    y += Math.sin(t * Math.PI * 20) * 30 * Math.exp(-t * 2);
                    y += Math.sin(t * Math.PI * 40) * 20 * Math.exp(-t * 3);
                    y += Math.sin(t * Math.PI * 60) * 15 * Math.exp(-t * 4);
                    
                    const finalY = centerY + y;
                    
                    if (x === 0) {
                        ctx.moveTo(x, finalY);
                    } else {
                        ctx.lineTo(x, finalY);
                    }
                }
                
                ctx.stroke();
            }
        }

        // Generate spectrogram visualization
        function generateSpectrogram() {
            const canvas = spectrogramCanvas;
            const ctx = canvas.getContext('2d');
            
            const containerWidth = canvas.parentElement.offsetWidth - 40;
            const containerHeight = canvas.parentElement.offsetHeight - 40;
            
            canvas.width = Math.max(containerWidth, 400);
            canvas.height = Math.max(containerHeight, 200);
            
            // Clear canvas
            ctx.fillStyle = '#1a202c';
            ctx.fillRect(0, 0, canvas.width, canvas.height);
            
            try {
                const imageData = ctx.createImageData(canvas.width, canvas.height);
                const data = imageData.data;
                
                for (let x = 0; x < canvas.width; x++) {
                    for (let y = 0; y < canvas.height; y++) {
                        const index = (y * canvas.width + x) * 4;
                        
                        const freq = 1 - (y / canvas.height);
                        const time = x / canvas.width;
                        
                        let intensity = 0;
                        
                        if (audioBuffer) {
                            const sampleIndex = Math.floor(time * audioBuffer.length);
                            const sample = audioBuffer.getChannelData(0)[sampleIndex] || 0;
                            
                            intensity += Math.abs(sample) * Math.exp(-Math.pow((freq - 0.3), 2) * 5);
                            intensity += Math.abs(sample) * 0.5 * Math.exp(-Math.pow((freq - 0.6), 2) * 8);
                            intensity += Math.abs(sample) * 0.3 * Math.exp(-Math.pow((freq - 0.9), 2) * 12);
                        } else {
                            intensity += Math.exp(-Math.pow((freq - 0.2), 2) * 20) * Math.sin(time * 10);
                            intensity += Math.exp(-Math.pow((freq - 0.5), 2) * 30) * Math.sin(time * 15);
                            intensity += Math.exp(-Math.pow((freq - 0.8), 2) * 40) * Math.sin(time * 5);
                        }
                        
                        intensity = Math.max(0, Math.min(1, Math.abs(intensity) * 2 + 0.1));
                        
                        if (intensity < 0.33) {
                            const t = intensity * 3;
                            data[index] = 0;
                            data[index + 1] = 0;
                            data[index + 2] = Math.floor(t * 255);
                        } else if (intensity < 0.66) {
                            const t = (intensity - 0.33) * 3;
                            data[index] = Math.floor(t * 255);
                            data[index + 1] = Math.floor(t * 255);
                            data[index + 2] = 255;
                        } else {
                            const t = (intensity - 0.66) * 3;
                            data[index] = 255;
                            data[index + 1] = Math.floor((1 - t) * 255);
                            data[index + 2] = 0;
                        }
                        data[index + 3] = 255;
                    }
                }
                
                ctx.putImageData(imageData, 0, 0);
            } catch (error) {
                console.error('Error creating spectrogram:', error);
            }
        }

        // Generate audio tokens
        function generateAudioTokens() {
            const duration = audioBuffer ? audioBuffer.duration : 4;
            const windowSize = 0.025;
            const hopSize = 0.01;
            
            const numWindows = Math.floor((duration - windowSize) / hopSize);
            const tokens = [];
            
            for (let i = 0; i < numWindows; i++) {
                const timePosition = i * hopSize / duration;
                
                if (audioBuffer) {
                    const sampleIndex = Math.floor(timePosition * audioBuffer.length);
                    const sample = audioBuffer.getChannelData(0)[sampleIndex] || 0;
                    const magnitude = Math.abs(sample);
                    
                    let baseToken;
                    if (magnitude > 0.1) {
                        baseToken = 2000 + Math.floor(magnitude * 8000);
                    } else if (magnitude > 0.02) {
                        baseToken = 500 + Math.floor(magnitude * 3000);
                    } else {
                        baseToken = 50 + Math.floor(magnitude * 200);
                    }
                    
                    const variation = Math.floor((Math.random() - 0.5) * 500);
                    tokens.push(Math.max(50, Math.min(15000, baseToken + variation)));
                } else {
                    const pattern = Math.sin(timePosition * 8) * 0.5 + 0.5;
                    const baseToken = 1000 + Math.floor(pattern * 8000);
                    tokens.push(baseToken);
                }
            }
            
            const displayTokens = tokens.slice(0, 80);
            const tokenString = `[${displayTokens.join(', ')}${tokens.length > 80 ? ', ...' : ''}]`;
            
            tokenArrayDiv.innerHTML = `
                <div style="margin-bottom: 10px;">
                    <strong>Speech Audio Tokens (${tokens.length} total, showing first ${Math.min(80, tokens.length)}):</strong>
                </div>
                ${tokenString}
                <div style="margin-top: 15px; font-size: 12px; color: #a0aec0;">
                    • Each token represents ~${(windowSize * 1000).toFixed(0)}ms of speech features<br>
                    • Token range: ${Math.min(...tokens)} - ${Math.max(...tokens)}<br>
                    • Audio duration: ${duration.toFixed(2)}s → ${tokens.length} tokens<br>
                    • High values (~2000+): Voiced speech (vowels)<br>
                    • Medium values (~500-2000): Unvoiced speech (consonants)<br>
                    • Low values (~50-500): Silence/background
                </div>
            `;
        }

        // Update audio status message
        function updateAudioStatus(message, isError = false) {
            const statusDiv = document.getElementById('audioStatus');
            statusDiv.textContent = message;
            statusDiv.style.color = isError ? '#dc3545' : '#666';
        }

        // Event Listeners
        analyzeBtn.addEventListener('click', async () => {
            debugLog('🔧 Analyze button clicked');
            analyzeBtn.disabled = true;
            analyzeBtn.textContent = '🔄 Analyzing...';
            
            try {
                debugLog('🔧 Starting analysis...');
                updateAudioStatus('🔄 Analyzing female speech audio...');
                
                debugLog('🔧 Calling loadAudio()...');
                await loadAudio();
                
                debugLog(`🔧 loadAudio() completed, audioBuffer: ${audioBuffer ? 'exists' : 'null'}`);
                
                updateAudioStatus('✅ Analysis complete! Waveform and spectrogram updated.');
                analyzeBtn.textContent = '✅ Analysis Complete';
                
                setTimeout(() => {
                    analyzeBtn.textContent = '🎯 Analyze Audio';
                    analyzeBtn.disabled = false;
                    updateAudioStatus('✅ Female speech loaded! Click play to listen');
                }, 2000);
            } catch (error) {
                debugLog(`❌ Analysis failed: ${error.name} - ${error.message}`);
                analyzeBtn.textContent = '❌ Analysis Failed';
                updateAudioStatus('❌ Analysis failed. Check debug console below.', true);
                analyzeBtn.disabled = false;
            }
        });

        generateTokensBtn.addEventListener('click', () => {
            generateTokensBtn.disabled = true;
            generateTokensBtn.textContent = '🔄 Generating...';
            
            setTimeout(() => {
                generateAudioTokens();
                generateTokensBtn.textContent = '✅ Tokens Generated';
                
                setTimeout(() => {
                    generateTokensBtn.textContent = '🔢 Generate Tokens';
                    generateTokensBtn.disabled = false;
                }, 2000);
            }, 1000);
        });

        // Audio loading status
        audioPlayer.addEventListener('loadeddata', () => {
            updateAudioStatus('✅ Female speech sample ready! Click play button above');
        });

        audioPlayer.addEventListener('loadstart', () => {
            updateAudioStatus('🔄 Loading female speech sample...');
        });

        audioPlayer.addEventListener('error', (e) => {
            updateAudioStatus('❌ Could not load from samkotey.dev. Check CORS settings.', true);
            console.error('Audio error:', e);
        });

        audioPlayer.addEventListener('canplay', () => {
            updateAudioStatus('✅ Female speech loaded! Click play to listen');
        });

        // Initialize with sample visualizations
        window.addEventListener('load', () => {
            debugLog('🔧 Page loaded, initializing...');
            setTimeout(() => {
                drawWaveform();
                generateSpectrogram();
                debugLog('✅ Initial visualizations drawn');
            }, 100);
        });

        // Handle canvas resize
        window.addEventListener('resize', () => {
            drawWaveform();
            generateSpectrogram();
        });
    </script>
</body>
</html>
