generateTokensBtn.addEventListener('click', () => {        // Initialize live audio visualization
        function initLiveVisualization() {        // Test button event listener
        testAudioBtn.addEventListener('click', () => {
            testAudioSupport();
        });

        // Fallback: Start simulation if real audio fails
        setTimeout(() => {
            if (!isVisualizationActive) {
                debugLog('üé¨ No real audio detected, starting simulation...');
                simulateLiveVisualization();
            }
        }, 3000);
            debugLog('üî¢ Generate tokens clicked');
            generateTokensBtn.disabled = true;
            generateTokensBtn.textContent = 'üîÑ Generating...';
            
            setTimeout(() => {
                generateAudioTokens();
                generateTokensBtn.textContent = '‚úÖ Tokens Generated';
                
                setTimeout(() => {
                    generateTokensBtn.textContent = 'üî¢ Generate Tokens';
                    generateTokensBtn.disabled = false;
                }, 2000);
            }, 1000);
        });

        // Audio loading status with debugging
        audioPlayer.addEventListener('loadstart', () => {
            debugLog('üì° Audio loadstart event');
            updateAudioStatus('üîÑ Loading test audio from public sources...');
        });

        audioPlayer.addEventListener('error', (e) => {
            debugLog(`‚ùå Audio error event: ${e.type}`);
            if (audioPlayer.error) {
                debugLog(`‚ùå MediaError: code=${audioPlayer.error.code}, message="${audioPlayer.error.message}"`);
            }
            updateAudioStatus('‚ùå Could not load test audio. Trying next source...', true);
            console.error('Audio error:', e);
        });<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How AI Hears Audio</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Helvetica', 'Arial', sans-serif;
            background: #ffffff;
            margin: 0;
            padding: 20px;
            color: #333;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            background: #ffffff;
        }

        h1 {
            text-align: center;
            margin-bottom: 30px;
            font-size: 2em;
            color: #333;
        }

        .description {
            text-align: center;
            color: #666;
            margin-bottom: 30px;
            font-size: 16px;
            line-height: 1.5;
        }

        .audio-section {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
        }

        .section-title {
            font-weight: bold;
            margin-bottom: 15px;
            color: #333;
            font-size: 18px;
        }

        .audio-player {
            width: 100%;
            margin-bottom: 15px;
        }

        .audio-info {
            font-size: 14px;
            color: #666;
            text-align: center;
        }

        .waveform-container {
            background: #2d3748;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            height: 200px;
            position: relative;
            overflow: hidden;
        }

        .waveform-canvas {
            width: 100%;
            height: 100%;
            background: #1a202c;
            border-radius: 4px;
        }

        .spectrogram-container {
            background: #2d3748;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            height: 300px;
            position: relative;
            overflow: hidden;
        }

        .spectrogram-canvas {
            width: 100%;
            height: 100%;
            background: #1a202c;
            border-radius: 4px;
        }

        .token-section {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
        }

        .token-array {
            background: #2d3748;
            color: #e2e8f0;
            border: 1px solid #4a5568;
            border-radius: 8px;
            padding: 15px;
            font-family: 'Monaco', 'Menlo', 'Consolas', monospace;
            font-size: 13px;
            line-height: 1.4;
            overflow-x: auto;
            white-space: pre-wrap;
            word-break: break-all;
            margin-bottom: 15px;
        }

        .token-explanation {
            font-size: 14px;
            color: #666;
            background: #f0f8ff;
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #007bff;
        }

        .progress-indicator {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 20px 0;
            font-size: 14px;
            color: #666;
        }

        .progress-step {
            display: flex;
            flex-direction: column;
            align-items: center;
            flex: 1;
            position: relative;
        }

        .progress-step:not(:last-child)::after {
            content: '‚Üí';
            position: absolute;
            right: -20px;
            top: 15px;
            font-size: 20px;
            color: #007bff;
        }

        .step-icon {
            width: 30px;
            height: 30px;
            border-radius: 50%;
            background: #007bff;
            color: white;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 5px;
            font-weight: bold;
        }

        .step-label {
            text-align: center;
            font-size: 12px;
        }

        .controls {
            text-align: center;
            margin: 20px 0;
        }

        .btn {
            background: #007bff;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 14px;
            margin: 0 5px;
            transition: background 0.2s;
        }

        .btn:hover {
            background: #0056b3;
        }

        .btn:disabled {
            background: #6c757d;
            cursor: not-allowed;
        }

        @media (max-width: 768px) {
            .container {
                padding: 10px;
            }
            
            .progress-indicator {
                flex-direction: column;
                gap: 15px;
            }
            
            .progress-step:not(:last-child)::after {
                content: '‚Üì';
                position: static;
                margin: 10px 0;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>How AI Hears Audio</h1>
        
        <div class="description">
            See how AI converts sound waves into numbers, just like it does with text. 
            This shows the journey from audio ‚Üí spectrogram ‚Üí tokens.
        </div>

        <div class="progress-indicator">
            <div class="progress-step">
                <div class="step-icon">üéµ</div>
                <div class="step-label">Audio Wave</div>
            </div>
            <div class="progress-step">
                <div class="step-icon">üìä</div>
                <div class="step-label">Spectrogram</div>
            </div>
            <div class="progress-step">
                <div class="step-icon">üî¢</div>
                <div class="step-label">Tokens</div>
            </div>
        </div>

        <div class="audio-section">
            <div class="section-title">1. Audio Player</div>
            <audio id="audioPlayer" class="audio-player" controls>
                <source src="https://www.soundjay.com/misc/sounds/bell-ringing-05.mp3" type="audio/mpeg">
                <source src="https://file-examples.com/storage/fe86c2b4d407299c2517ebc/2017/11/file_example_MP3_1MG.mp3" type="audio/mpeg">
                <source src="https://sample-videos.com/zip/10/mp3/mp3-sample.mp3" type="audio/mpeg">
                Your browser does not support the audio element.
            </audio>
            <div class="audio-info">
                <div id="audioStatus">Testing audio in Claude environment...</div>
                <div style="font-size: 12px; margin-top: 5px; color: #ff6b6b;">
                    ‚ö†Ô∏è Claude may block audio playback. For full functionality, copy this code to your own webpage.
                </div>
                <button id="testAudioBtn" class="btn" style="margin-top: 10px;">üß™ Test Audio Support</button>
            </div>
        </div>

        <div class="controls">
            <button id="generateTokensBtn" class="btn">üî¢ Generate Tokens</button>
            <div style="margin-top: 10px; font-size: 14px; color: #666;">
                ‚ñ∂Ô∏è Play the audio above to see live waveform and spectrogram visualization
            </div>
        </div>

        <div class="audio-section">
            <div class="section-title">2. Waveform (Time Domain)</div>
            <div class="waveform-container">
                <canvas id="waveformCanvas" class="waveform-canvas"></canvas>
            </div>
        </div>

        <div class="audio-section">
            <div class="section-title">3. Spectrogram (Frequency Domain)</div>
            <div class="spectrogram-container">
                <canvas id="spectrogramCanvas" class="spectrogram-canvas"></canvas>
            </div>
        </div>

        <div class="token-section">
            <div class="section-title">4. Audio Tokens (What AI Actually Processes)</div>
            <div id="tokenArray" class="token-array">
                Click "Analyze Audio" to see how the audio gets converted to numbers...
            </div>
            <div class="token-explanation">
                <strong>How Audio Tokenization Works:</strong><br>
                ‚Ä¢ The spectrogram is divided into time segments (windows)<br>
                ‚Ä¢ Each window's frequency data becomes a feature vector<br>
                ‚Ä¢ These vectors are quantized into discrete tokens<br>
                ‚Ä¢ AI models like Whisper process these token sequences<br>
                ‚Ä¢ Similar to text tokens, but representing audio features instead of words
            </div>
        </div>

        <div class="token-section">
            <div class="section-title">5. Debug Console</div>
            <div id="debugConsole" style="background: #000; color: #0f0; font-family: monospace; font-size: 12px; padding: 15px; border-radius: 8px; height: 200px; overflow-y: auto; white-space: pre-wrap;">
                Debug messages will appear here...
            </div>
        </div>
    </div>

    <script>
        let audioContext;
        let audioBuffer;
        let analyser;
        let source;
        let animationId;
        let isVisualizationActive = false;

        const audioPlayer = document.getElementById('audioPlayer');
        const generateTokensBtn = document.getElementById('generateTokensBtn');
        const testAudioBtn = document.getElementById('testAudioBtn');
        const waveformCanvas = document.getElementById('waveformCanvas');
        const spectrogramCanvas = document.getElementById('spectrogramCanvas');
        const tokenArrayDiv = document.getElementById('tokenArray');
        const debugConsole = document.getElementById('debugConsole');

        // Debug logging function
        function debugLog(message) {
            const timestamp = new Date().toLocaleTimeString();
            const logMessage = `[${timestamp}] ${message}\n`;
            debugConsole.textContent += logMessage;
            debugConsole.scrollTop = debugConsole.scrollHeight;
            console.log(message);
        }

        // Initialize audio context on user interaction
        function initAudioContext() {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
            }
        }

        // Test audio and Web Audio API support
        function testAudioSupport() {
            debugLog('üß™ Testing audio support in this environment...');
            
            // Test 1: Audio element
            debugLog(`üß™ Audio element support: ${!!window.HTMLAudioElement}`);
            debugLog(`üß™ Audio player element exists: ${!!audioPlayer}`);
            debugLog(`üß™ Audio player src: ${audioPlayer.src}`);
            debugLog(`üß™ Audio player ready state: ${audioPlayer.readyState}`);
            debugLog(`üß™ Audio player network state: ${audioPlayer.networkState}`);
            
            // Test 2: Web Audio API
            debugLog(`üß™ AudioContext support: ${!!(window.AudioContext || window.webkitAudioContext)}`);
            
            try {
                const testContext = new (window.AudioContext || window.webkitAudioContext)();
                debugLog(`üß™ AudioContext creation: SUCCESS, state=${testContext.state}`);
                testContext.close();
            } catch (error) {
                debugLog(`üß™ AudioContext creation: FAILED - ${error.message}`);
            }
            
            // Test 3: Try to play audio
            debugLog('üß™ Attempting to play audio...');
            const playPromise = audioPlayer.play();
            
            if (playPromise !== undefined) {
                playPromise
                    .then(() => {
                        debugLog('üß™ Audio play: SUCCESS');
                        audioPlayer.pause();
                    })
                    .catch(error => {
                        debugLog(`üß™ Audio play: FAILED - ${error.name}: ${error.message}`);
                    });
            } else {
                debugLog('üß™ Audio play: No promise returned (old browser?)');
            }
            
            // Test 4: Canvas support
            debugLog(`üß™ Canvas support: ${!!window.HTMLCanvasElement}`);
            
            try {
                const testCanvas = document.createElement('canvas');
                const testCtx = testCanvas.getContext('2d');
                debugLog(`üß™ Canvas 2D context: ${!!testCtx}`);
            } catch (error) {
                debugLog(`üß™ Canvas test: FAILED - ${error.message}`);
            }
            
            // Test 5: Check for iframe restrictions
            debugLog(`üß™ Window parent: ${window.parent === window ? 'STANDALONE' : 'IFRAME'}`);
            debugLog(`üß™ Document domain: ${document.domain}`);
            debugLog(`üß™ Location protocol: ${window.location.protocol}`);
        }

        // Simulate live visualization with fake data
        function simulateLiveVisualization() {
            debugLog('üé¨ Starting simulated live visualization...');
            isVisualizationActive = true;
            
            let frameCount = 0;
            
            function animateSimulation() {
                if (!isVisualizationActive) return;
                
                frameCount++;
                
                // Generate fake audio data
                const fakeTimeData = new Uint8Array(1024);
                const fakeFreqData = new Uint8Array(512);
                
                for (let i = 0; i < fakeTimeData.length; i++) {
                    fakeTimeData[i] = 128 + Math.sin(frameCount * 0.1 + i * 0.01) * 30;
                }
                
                for (let i = 0; i < fakeFreqData.length; i++) {
                    fakeFreqData[i] = Math.random() * 100 + Math.sin(frameCount * 0.05 + i * 0.02) * 50;
                }
                
                // Draw with fake data
                drawLiveWaveform(fakeTimeData);
                drawLiveSpectrogram(fakeFreqData);
                
                if (frameCount % 60 === 0) { // Every second
                    debugLog(`üé¨ Simulation frame ${frameCount}, fake data generated`);
                }
                
                requestAnimationFrame(animateSimulation);
            }
            
            animateSimulation();
            updateAudioStatus('üé¨ Running simulated visualization (no real audio)');
        }
            try {
                debugLog('üîß Initializing live visualization...');
                
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    debugLog(`üîß Audio context created, state: ${audioContext.state}`);
                }

                if (audioContext.state === 'suspended') {
                    audioContext.resume();
                    debugLog('üîß Audio context resumed');
                }

                // Create analyser node
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 2048;
                analyser.smoothingTimeConstant = 0.8;
                
                debugLog(`üîß Analyser created - FFT size: ${analyser.fftSize}`);

                // Connect audio element to analyser
                if (!source) {
                    source = audioContext.createMediaElementSource(audioPlayer);
                    source.connect(analyser);
                    analyser.connect(audioContext.destination);
                    debugLog('üîß Audio source connected to analyser');
                }

                isVisualizationActive = true;
                startVisualization();
                
            } catch (error) {
                debugLog(`‚ùå Visualization init failed: ${error.message}`);
                // Fall back to static visualizations
                drawStaticVisualizations();
            }
        }

        // Start the visualization loop
        function startVisualization() {
            if (!isVisualizationActive) return;

            // Get frequency and time domain data
            const bufferLength = analyser.frequencyBinCount;
            const frequencyData = new Uint8Array(bufferLength);
            const timeDomainData = new Uint8Array(bufferLength);
            
            analyser.getByteFrequencyData(frequencyData);
            analyser.getByteTimeDomainData(timeDomainData);

            // Draw live waveform
            drawLiveWaveform(timeDomainData);
            
            // Draw live spectrogram
            drawLiveSpectrogram(frequencyData);

            // Continue animation
            animationId = requestAnimationFrame(startVisualization);
        }

        // Stop visualization
        function stopVisualization() {
            isVisualizationActive = false;
            if (animationId) {
                cancelAnimationFrame(animationId);
            }
            debugLog('üîß Live visualization stopped');
        }

        // Draw live waveform
        function drawLiveWaveform(timeDomainData) {
            const canvas = waveformCanvas;
            const ctx = canvas.getContext('2d');
            
            // Set canvas size
            const container = canvas.parentElement;
            canvas.width = Math.max(container.clientWidth - 40, 600);
            canvas.height = Math.max(container.clientHeight - 40, 150);

            // Clear canvas
            ctx.fillStyle = '#1a202c';
            ctx.fillRect(0, 0, canvas.width, canvas.height);

            // Draw waveform
            ctx.strokeStyle = '#00d4ff';
            ctx.lineWidth = 2;
            ctx.beginPath();

            const sliceWidth = canvas.width / timeDomainData.length;
            let x = 0;

            for (let i = 0; i < timeDomainData.length; i++) {
                const v = timeDomainData[i] / 128.0; // Convert to 0-2 range
                const y = (v * canvas.height) / 2;

                if (i === 0) {
                    ctx.moveTo(x, y);
                } else {
                    ctx.lineTo(x, y);
                }

                x += sliceWidth;
            }

            ctx.stroke();
        }

        // Draw live spectrogram
        function drawLiveSpectrogram(frequencyData) {
            const canvas = spectrogramCanvas;
            const ctx = canvas.getContext('2d');
            
            // Set canvas size
            const container = canvas.parentElement;
            canvas.width = Math.max(container.clientWidth - 40, 600);
            canvas.height = Math.max(container.clientHeight - 40, 250);

            // Shift existing data left by 1 pixel
            const imageData = ctx.getImageData(1, 0, canvas.width - 1, canvas.height);
            ctx.putImageData(imageData, 0, 0);

            // Draw new frequency column on the right
            const x = canvas.width - 1;
            
            for (let y = 0; y < canvas.height; y++) {
                // Map y position to frequency bin
                const freqIndex = Math.floor((canvas.height - y) / canvas.height * frequencyData.length);
                const intensity = frequencyData[freqIndex] / 255.0;

                // Color mapping based on intensity
                let r, g, b;
                if (intensity < 0.2) {
                    // Black to blue
                    const t = intensity * 5;
                    r = 0;
                    g = 0;
                    b = Math.floor(t * 255);
                } else if (intensity < 0.4) {
                    // Blue to cyan
                    const t = (intensity - 0.2) * 5;
                    r = 0;
                    g = Math.floor(t * 255);
                    b = 255;
                } else if (intensity < 0.6) {
                    // Cyan to yellow
                    const t = (intensity - 0.4) * 5;
                    r = Math.floor(t * 255);
                    g = 255;
                    b = 255 - Math.floor(t * 255);
                } else if (intensity < 0.8) {
                    // Yellow to orange
                    const t = (intensity - 0.6) * 5;
                    r = 255;
                    g = 255 - Math.floor(t * 128);
                    b = 0;
                } else {
                    // Orange to red
                    const t = (intensity - 0.8) * 5;
                    r = 255;
                    g = 127 - Math.floor(t * 127);
                    b = 0;
                }

                ctx.fillStyle = `rgb(${r},${g},${b})`;
                ctx.fillRect(x, y, 1, 1);
            }
        }

        // Draw static visualizations when live fails
        function drawStaticVisualizations() {
            debugLog('üîß Drawing static visualizations...');
            
            // Static waveform
            const waveCanvas = waveformCanvas;
            const waveCtx = waveCanvas.getContext('2d');
            waveCanvas.width = Math.max(waveCanvas.parentElement.clientWidth - 40, 600);
            waveCanvas.height = 150;
            
            waveCtx.fillStyle = '#1a202c';
            waveCtx.fillRect(0, 0, waveCanvas.width, waveCanvas.height);
            
            waveCtx.strokeStyle = '#00d4ff';
            waveCtx.lineWidth = 2;
            waveCtx.beginPath();
            
            const centerY = waveCanvas.height / 2;
            for (let x = 0; x < waveCanvas.width; x++) {
                const t = x / waveCanvas.width;
                const y = centerY + Math.sin(t * Math.PI * 10) * 30 * Math.sin(t * Math.PI * 2);
                if (x === 0) waveCtx.moveTo(x, y);
                else waveCtx.lineTo(x, y);
            }
            waveCtx.stroke();

            // Static spectrogram
            const specCanvas = spectrogramCanvas;
            const specCtx = specCanvas.getContext('2d');
            specCanvas.width = Math.max(specCanvas.parentElement.clientWidth - 40, 600);
            specCanvas.height = 250;
            
            const gradient = specCtx.createLinearGradient(0, 0, specCanvas.width, specCanvas.height);
            gradient.addColorStop(0, '#000080');
            gradient.addColorStop(0.3, '#0080ff');
            gradient.addColorStop(0.6, '#ffff00');
            gradient.addColorStop(1, '#ff0000');
            specCtx.fillStyle = gradient;
            specCtx.fillRect(0, 0, specCanvas.width, specCanvas.height);
        }

        // Draw waveform visualization
        function drawWaveform() {
            const canvas = waveformCanvas;
            const ctx = canvas.getContext('2d');
            
            // Set canvas dimensions properly
            const container = canvas.parentElement;
            const containerWidth = container.clientWidth - 40; // Account for padding
            const containerHeight = container.clientHeight - 40;
            
            // Set actual canvas size
            canvas.width = Math.max(containerWidth, 600);
            canvas.height = Math.max(containerHeight, 150);
            
            debugLog(`üîß Waveform canvas: ${canvas.width}x${canvas.height}`);
            
            // Clear canvas with dark background
            ctx.fillStyle = '#1a202c';
            ctx.fillRect(0, 0, canvas.width, canvas.height);
            
            if (audioBuffer && audioBuffer.getChannelData) {
                debugLog('üîß Drawing real waveform data');
                try {
                    // Use actual audio data
                    const data = audioBuffer.getChannelData(0);
                    const step = Math.ceil(data.length / canvas.width);
                    const amp = canvas.height / 2;
                    
                    ctx.strokeStyle = '#00d4ff';
                    ctx.lineWidth = 2;
                    ctx.beginPath();
                    
                    for (let i = 0; i < canvas.width; i++) {
                        const start = i * step;
                        const end = Math.min(start + step, data.length);
                        
                        if (end > start) {
                            const slice = Array.from(data.slice(start, end));
                            const min = Math.min(...slice);
                            const max = Math.max(...slice);
                            
                            ctx.moveTo(i, (1 + min) * amp);
                            ctx.lineTo(i, (1 + max) * amp);
                        }
                    }
                    
                    ctx.stroke();
                    debugLog('‚úÖ Real waveform drawn');
                } catch (error) {
                    debugLog(`‚ùå Waveform error: ${error.message}`);
                    drawFallbackWaveform(ctx, canvas);
                }
            } else {
                debugLog('üîß Drawing fallback waveform');
                drawFallbackWaveform(ctx, canvas);
            }
        }

        // Draw fallback waveform
        function drawFallbackWaveform(ctx, canvas) {
            ctx.strokeStyle = '#00d4ff';
            ctx.lineWidth = 2;
            ctx.beginPath();
            
            const centerY = canvas.height / 2;
            const duration = audioPlayer.duration || 4;
            
            for (let x = 0; x < canvas.width; x++) {
                const t = (x / canvas.width) * duration;
                
                // Create speech-like waveform
                let y = 0;
                
                // Add multiple frequency components
                y += Math.sin(2 * Math.PI * 2 * t) * 40; // Low frequency
                y += Math.sin(2 * Math.PI * 8 * t) * 20; // Mid frequency  
                y += Math.sin(2 * Math.PI * 15 * t) * 10; // High frequency
                
                // Add envelope for speech-like pattern
                const envelope = Math.sin((t / duration) * Math.PI * 6) * Math.exp(-(t % 1) * 2);
                y *= envelope;
                
                const finalY = centerY + y;
                
                if (x === 0) {
                    ctx.moveTo(x, finalY);
                } else {
                    ctx.lineTo(x, finalY);
                }
            }
            
            ctx.stroke();
            debugLog('‚úÖ Fallback waveform drawn');
        }

        // Generate spectrogram visualization
        function generateSpectrogram() {
            const canvas = spectrogramCanvas;
            const ctx = canvas.getContext('2d');
            
            // Set canvas dimensions properly
            const container = canvas.parentElement;
            const containerWidth = container.clientWidth - 40;
            const containerHeight = container.clientHeight - 40;
            
            canvas.width = Math.max(containerWidth, 600);
            canvas.height = Math.max(containerHeight, 250);
            
            debugLog(`üîß Spectrogram canvas: ${canvas.width}x${canvas.height}`);
            
            // Clear canvas
            ctx.fillStyle = '#1a202c';
            ctx.fillRect(0, 0, canvas.width, canvas.height);
            
            try {
                const imageData = ctx.createImageData(canvas.width, canvas.height);
                const data = imageData.data;
                const duration = audioBuffer ? audioBuffer.duration : (audioPlayer.duration || 4);
                
                debugLog(`üîß Creating spectrogram for ${duration}s audio`);
                
                for (let x = 0; x < canvas.width; x++) {
                    for (let y = 0; y < canvas.height; y++) {
                        const index = (y * canvas.width + x) * 4;
                        
                        // Frequency (y-axis): 0 = high freq, canvas.height = low freq
                        const freq = 1 - (y / canvas.height);
                        // Time (x-axis)
                        const time = x / canvas.width;
                        
                        let intensity = 0;
                        
                        if (audioBuffer && audioBuffer.getChannelData) {
                            // Use real audio data if available
                            try {
                                const sampleIndex = Math.floor(time * audioBuffer.length);
                                const sample = audioBuffer.getChannelData(0)[sampleIndex] || 0;
                                
                                // Simulate frequency analysis
                                intensity += Math.abs(sample) * Math.exp(-Math.pow((freq - 0.2), 2) * 8);
                                intensity += Math.abs(sample) * 0.7 * Math.exp(-Math.pow((freq - 0.5), 2) * 10);
                                intensity += Math.abs(sample) * 0.4 * Math.exp(-Math.pow((freq - 0.8), 2) * 15);
                            } catch (e) {
                                // Fallback if real data fails
                                intensity = createFallbackIntensity(freq, time, duration);
                            }
                        } else {
                            // Create realistic pattern
                            intensity = createFallbackIntensity(freq, time, duration);
                        }
                        
                        // Clamp intensity
                        intensity = Math.max(0, Math.min(1, intensity * 3 + 0.1));
                        
                        // Color mapping: black -> blue -> cyan -> yellow -> red
                        if (intensity < 0.2) {
                            // Black to dark blue
                            const t = intensity * 5;
                            data[index] = 0;
                            data[index + 1] = 0;
                            data[index + 2] = Math.floor(t * 128);
                        } else if (intensity < 0.4) {
                            // Dark blue to bright blue
                            const t = (intensity - 0.2) * 5;
                            data[index] = 0;
                            data[index + 1] = Math.floor(t * 100);
                            data[index + 2] = 128 + Math.floor(t * 127);
                        } else if (intensity < 0.6) {
                            // Blue to cyan
                            const t = (intensity - 0.4) * 5;
                            data[index] = 0;
                            data[index + 1] = 100 + Math.floor(t * 155);
                            data[index + 2] = 255;
                        } else if (intensity < 0.8) {
                            // Cyan to yellow
                            const t = (intensity - 0.6) * 5;
                            data[index] = Math.floor(t * 255);
                            data[index + 1] = 255;
                            data[index + 2] = 255 - Math.floor(t * 255);
                        } else {
                            // Yellow to red
                            const t = (intensity - 0.8) * 5;
                            data[index] = 255;
                            data[index + 1] = 255 - Math.floor(t * 255);
                            data[index + 2] = 0;
                        }
                        data[index + 3] = 255; // Alpha
                    }
                }
                
                ctx.putImageData(imageData, 0, 0);
                debugLog('‚úÖ Spectrogram generated');
            } catch (error) {
                debugLog(`‚ùå Spectrogram error: ${error.message}`);
                // Simple fallback pattern
                const gradient = ctx.createLinearGradient(0, 0, canvas.width, canvas.height);
                gradient.addColorStop(0, '#000080');
                gradient.addColorStop(0.5, '#0080ff');
                gradient.addColorStop(1, '#ff8000');
                ctx.fillStyle = gradient;
                ctx.fillRect(0, 0, canvas.width, canvas.height);
            }
        }

        // Create fallback intensity pattern
        function createFallbackIntensity(freq, time, duration) {
            // Create speech-like frequency patterns
            let intensity = 0;
            
            // Fundamental frequency around 150Hz (normalized to ~0.15)
            intensity += Math.exp(-Math.pow((freq - 0.15), 2) * 20) * Math.sin(time * duration * 2);
            
            // First formant around 500Hz (normalized to ~0.5)  
            intensity += Math.exp(-Math.pow((freq - 0.5), 2) * 15) * Math.sin(time * duration * 3);
            
            // Second formant around 1200Hz (normalized to ~0.8)
            intensity += Math.exp(-Math.pow((freq - 0.8), 2) * 25) * Math.sin(time * duration * 1.5);
            
            // Add some time-varying envelope
            intensity *= (0.5 + 0.5 * Math.sin(time * Math.PI * 4));
            
            return Math.abs(intensity);
        }

        // Generate audio tokens
        function generateAudioTokens() {
            const duration = audioBuffer ? audioBuffer.duration : 4;
            const windowSize = 0.025;
            const hopSize = 0.01;
            
            const numWindows = Math.floor((duration - windowSize) / hopSize);
            const tokens = [];
            
            for (let i = 0; i < numWindows; i++) {
                const timePosition = i * hopSize / duration;
                
                if (audioBuffer) {
                    const sampleIndex = Math.floor(timePosition * audioBuffer.length);
                    const sample = audioBuffer.getChannelData(0)[sampleIndex] || 0;
                    const magnitude = Math.abs(sample);
                    
                    let baseToken;
                    if (magnitude > 0.1) {
                        baseToken = 2000 + Math.floor(magnitude * 8000);
                    } else if (magnitude > 0.02) {
                        baseToken = 500 + Math.floor(magnitude * 3000);
                    } else {
                        baseToken = 50 + Math.floor(magnitude * 200);
                    }
                    
                    const variation = Math.floor((Math.random() - 0.5) * 500);
                    tokens.push(Math.max(50, Math.min(15000, baseToken + variation)));
                } else {
                    const pattern = Math.sin(timePosition * 8) * 0.5 + 0.5;
                    const baseToken = 1000 + Math.floor(pattern * 8000);
                    tokens.push(baseToken);
                }
            }
            
            const displayTokens = tokens.slice(0, 80);
            const tokenString = `[${displayTokens.join(', ')}${tokens.length > 80 ? ', ...' : ''}]`;
            
            tokenArrayDiv.innerHTML = `
                <div style="margin-bottom: 10px;">
                    <strong>Speech Audio Tokens (${tokens.length} total, showing first ${Math.min(80, tokens.length)}):</strong>
                </div>
                ${tokenString}
                <div style="margin-top: 15px; font-size: 12px; color: #a0aec0;">
                    ‚Ä¢ Each token represents ~${(windowSize * 1000).toFixed(0)}ms of speech features<br>
                    ‚Ä¢ Token range: ${Math.min(...tokens)} - ${Math.max(...tokens)}<br>
                    ‚Ä¢ Audio duration: ${duration.toFixed(2)}s ‚Üí ${tokens.length} tokens<br>
                    ‚Ä¢ High values (~2000+): Voiced speech (vowels)<br>
                    ‚Ä¢ Medium values (~500-2000): Unvoiced speech (consonants)<br>
                    ‚Ä¢ Low values (~50-500): Silence/background
                </div>
            `;
        }

        // Update audio status message
        function updateAudioStatus(message, isError = false) {
            const statusDiv = document.getElementById('audioStatus');
            statusDiv.textContent = message;
            statusDiv.style.color = isError ? '#dc3545' : '#666';
        }

        // Event Listeners for live visualization
        audioPlayer.addEventListener('play', () => {
            debugLog('‚ñ∂Ô∏è Audio started playing - initializing live visualization');
            debugLog(`üîß Audio player state: readyState=${audioPlayer.readyState}, paused=${audioPlayer.paused}, currentTime=${audioPlayer.currentTime}`);
            updateAudioStatus('üéµ Playing with live visualization');
            
            // Small delay to ensure audio is actually playing
            setTimeout(() => {
                debugLog('üîß Delayed initialization after play event');
                initLiveVisualization();
            }, 100);
        });

        audioPlayer.addEventListener('pause', () => {
            debugLog('‚è∏Ô∏è Audio paused - stopping visualization');
            stopVisualization();
            updateAudioStatus('‚è∏Ô∏è Paused');
        });

        audioPlayer.addEventListener('ended', () => {
            debugLog('üîö Audio ended - stopping visualization');
            stopVisualization();
            updateAudioStatus('‚úÖ Playback complete');
        });

        // Additional debug events
        audioPlayer.addEventListener('timeupdate', () => {
            if (Math.random() < 0.05) { // Log occasionally (5% chance)
                debugLog(`‚è∞ Time: ${audioPlayer.currentTime.toFixed(2)}s / ${audioPlayer.duration.toFixed(2)}s, Playing: ${!audioPlayer.paused}`);
            }
        });

        audioPlayer.addEventListener('volumechange', () => {
            debugLog(`üîä Volume: ${audioPlayer.volume}, Muted: ${audioPlayer.muted}`);
        });

        // Try to initialize on any user interaction with audio
        audioPlayer.addEventListener('loadeddata', () => {
            debugLog('üìÅ Audio loadeddata event - audio file successfully loaded');
            updateAudioStatus('‚úÖ Test audio ready! Click play to see live visualization');
        });

        audioPlayer.addEventListener('canplay', () => {
            debugLog('‚úÖ Audio canplay event - ready for playback');
            updateAudioStatus('‚úÖ Audio loaded! Click play to see live waveform and spectrogram');
        });

        // Initialize with static visualizations
        window.addEventListener('load', () => {
            debugLog('üîß Page loaded, initializing...');
            setTimeout(() => {
                drawStaticVisualizations();
                debugLog('‚úÖ Initial static visualizations drawn');
            }, 100);
        });

        // Handle canvas resize
        window.addEventListener('resize', () => {
            drawWaveform();
            generateSpectrogram();
        });
    </script>
</body>
</html>
