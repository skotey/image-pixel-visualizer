<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How AI Hears Audio</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Helvetica', 'Arial', sans-serif;
            background: #ffffff;
            margin: 0;
            padding: 20px;
            color: #333;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            background: #ffffff;
        }

        h1 {
            text-align: center;
            margin-bottom: 30px;
            font-size: 2em;
            color: #333;
        }

        .description {
            text-align: center;
            color: #666;
            margin-bottom: 30px;
            font-size: 16px;
            line-height: 1.5;
        }

        .audio-section {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
        }

        .section-title {
            font-weight: bold;
            margin-bottom: 15px;
            color: #333;
            font-size: 18px;
        }

        .audio-player {
            width: 100%;
            margin-bottom: 15px;
        }

        .audio-info {
            font-size: 14px;
            color: #666;
            text-align: center;
        }

        .waveform-container {
            background: #2d3748;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            height: 200px;
            position: relative;
            overflow: hidden;
        }

        .waveform-canvas {
            width: 100%;
            height: 100%;
            background: #1a202c;
            border-radius: 4px;
        }

        .spectrogram-container {
            background: #2d3748;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            height: 300px;
            position: relative;
            overflow: hidden;
        }

        .spectrogram-canvas {
            width: 100%;
            height: 100%;
            background: #1a202c;
            border-radius: 4px;
        }

        .token-section {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
        }

        .token-array {
            background: #2d3748;
            color: #e2e8f0;
            border: 1px solid #4a5568;
            border-radius: 8px;
            padding: 15px;
            font-family: 'Monaco', 'Menlo', 'Consolas', monospace;
            font-size: 13px;
            line-height: 1.4;
            overflow-x: auto;
            white-space: pre-wrap;
            word-break: break-all;
            margin-bottom: 15px;
        }

        .token-explanation {
            font-size: 14px;
            color: #666;
            background: #f0f8ff;
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #007bff;
        }

        .progress-indicator {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 20px 0;
            font-size: 14px;
            color: #666;
        }

        .progress-step {
            display: flex;
            flex-direction: column;
            align-items: center;
            flex: 1;
            position: relative;
        }

        .progress-step:not(:last-child)::after {
            content: '→';
            position: absolute;
            right: -20px;
            top: 15px;
            font-size: 20px;
            color: #007bff;
        }

        .step-icon {
            width: 30px;
            height: 30px;
            border-radius: 50%;
            background: #007bff;
            color: white;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 5px;
            font-weight: bold;
        }

        .step-label {
            text-align: center;
            font-size: 12px;
        }

        .controls {
            text-align: center;
            margin: 20px 0;
        }

        .btn {
            background: #007bff;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 14px;
            margin: 0 5px;
            transition: background 0.2s;
        }

        .btn:hover {
            background: #0056b3;
        }

        .btn:disabled {
            background: #6c757d;
            cursor: not-allowed;
        }

        @media (max-width: 768px) {
            .container {
                padding: 10px;
            }
            
            .progress-indicator {
                flex-direction: column;
                gap: 15px;
            }
            
            .progress-step:not(:last-child)::after {
                content: '↓';
                position: static;
                margin: 10px 0;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>How AI Hears Audio</h1>
        
        <div class="description">
            See how AI converts sound waves into numbers, just like it does with text. 
            This shows the journey from audio → spectrogram → tokens.
        </div>

        <div class="progress-indicator">
            <div class="progress-step">
                <div class="step-icon">🎵</div>
                <div class="step-label">Audio Wave</div>
            </div>
            <div class="progress-step">
                <div class="step-icon">📊</div>
                <div class="step-label">Spectrogram</div>
            </div>
            <div class="progress-step">
                <div class="step-icon">🔢</div>
                <div class="step-label">Tokens</div>
            </div>
        </div>

        <div class="audio-section">
            <div class="section-title">1. Audio Player</div>
            <audio id="audioPlayer" class="audio-player" controls>
                <source src="https://www.soundjay.com/misc/sounds/bell-ringing-05.mp3" type="audio/mpeg">
                <source src="https://file-examples.com/storage/fe86c2b4d407299c2517ebc/2017/11/file_example_MP3_1MG.mp3" type="audio/mpeg">
                Your browser does not support the audio element.
            </audio>
            <div class="audio-info">
                <div id="audioStatus">Click play to start live visualization</div>
                <div style="font-size: 12px; margin-top: 5px;">
                    Live waveform and spectrogram will appear when audio plays
                </div>
            </div>
        </div>

        <div class="controls">
            <button id="generateTokensBtn" class="btn">🔢 Generate Tokens</button>
            <button id="testAudioBtn" class="btn">🧪 Test Audio Support</button>
        </div>

        <div class="audio-section">
            <div class="section-title">2. Waveform (Time Domain)</div>
            <div class="waveform-container">
                <canvas id="waveformCanvas" class="waveform-canvas"></canvas>
            </div>
        </div>

        <div class="audio-section">
            <div class="section-title">3. Spectrogram (Frequency Domain)</div>
            <div class="spectrogram-container">
                <canvas id="spectrogramCanvas" class="spectrogram-canvas"></canvas>
            </div>
        </div>

        <div class="token-section">
            <div class="section-title">4. Audio Tokens (What AI Actually Processes)</div>
            <div id="tokenArray" class="token-array">
                Click "Generate Tokens" to see how audio gets converted to numbers...
            </div>
            <div class="token-explanation">
                <strong>How Audio Tokenization Works:</strong><br>
                • The spectrogram is divided into time segments (windows)<br>
                • Each window's frequency data becomes a feature vector<br>
                • These vectors are quantized into discrete tokens<br>
                • AI models like Whisper process these token sequences<br>
                • Similar to text tokens, but representing audio features instead of words
            </div>
        </div>

        <div class="token-section">
            <div class="section-title">5. Debug Console</div>
            <div id="debugConsole" style="background: #000; color: #0f0; font-family: monospace; font-size: 12px; padding: 15px; border-radius: 8px; height: 200px; overflow-y: auto; white-space: pre-wrap;">
Ready for testing...
            </div>
        </div>
    </div>

    <script>
        let audioContext;
        let analyser;
        let source;
        let animationId;
        let isVisualizationActive = false;

        const audioPlayer = document.getElementById('audioPlayer');
        const generateTokensBtn = document.getElementById('generateTokensBtn');
        const testAudioBtn = document.getElementById('testAudioBtn');
        const waveformCanvas = document.getElementById('waveformCanvas');
        const spectrogramCanvas = document.getElementById('spectrogramCanvas');
        const tokenArrayDiv = document.getElementById('tokenArray');
        const debugConsole = document.getElementById('debugConsole');

        // Debug logging function
        function debugLog(message) {
            const timestamp = new Date().toLocaleTimeString();
            const logMessage = `[${timestamp}] ${message}\n`;
            debugConsole.textContent += logMessage;
            debugConsole.scrollTop = debugConsole.scrollHeight;
            console.log(message);
        }

        // Test audio and Web Audio API support
        function testAudioSupport() {
            debugLog('🧪 Testing audio support...');
            
            debugLog(`🧪 HTMLAudioElement: ${!!window.HTMLAudioElement}`);
            debugLog(`🧪 AudioContext: ${!!(window.AudioContext || window.webkitAudioContext)}`);
            debugLog(`🧪 Audio src: ${audioPlayer.src}`);
            debugLog(`🧪 Ready state: ${audioPlayer.readyState}`);
            debugLog(`🧪 Environment: ${window.parent === window ? 'STANDALONE' : 'IFRAME'}`);
            
            try {
                const testContext = new (window.AudioContext || window.webkitAudioContext)();
                debugLog(`🧪 AudioContext created: ${testContext.state}`);
                testContext.close();
            } catch (error) {
                debugLog(`🧪 AudioContext failed: ${error.message}`);
            }
        }

        // Initialize live audio visualization
        function initLiveVisualization() {
            try {
                debugLog('🔧 Initializing live visualization...');
                
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    debugLog(`🔧 AudioContext created: ${audioContext.state}`);
                }

                if (audioContext.state === 'suspended') {
                    audioContext.resume().then(() => {
                        debugLog(`🔧 AudioContext resumed: ${audioContext.state}`);
                    });
                }

                if (!analyser) {
                    analyser = audioContext.createAnalyser();
                    analyser.fftSize = 2048;
                    analyser.smoothingTimeConstant = 0.8;
                    debugLog(`🔧 Analyser created: FFT=${analyser.fftSize}`);
                }

                if (!source) {
                    source = audioContext.createMediaElementSource(audioPlayer);
                    source.connect(analyser);
                    analyser.connect(audioContext.destination);
                    debugLog('🔧 Audio pipeline: element → analyser → destination');
                }

                isVisualizationActive = true;
                startVisualizationLoop();
                
            } catch (error) {
                debugLog(`❌ Init failed: ${error.message}`);
                startSimulation();
            }
        }

        // Start the visualization loop
        function startVisualizationLoop() {
            if (!isVisualizationActive || !analyser) return;

            const bufferLength = analyser.frequencyBinCount;
            const frequencyData = new Uint8Array(bufferLength);
            const timeDomainData = new Uint8Array(bufferLength);
            
            analyser.getByteFrequencyData(frequencyData);
            analyser.getByteTimeDomainData(timeDomainData);

            // Check for actual data
            const freqSum = frequencyData.reduce((a, b) => a + b, 0);
            const timeSum = timeDomainData.reduce((a, b) => a + b, 0);
            
            if (Math.random() < 0.02) {
                debugLog(`📊 Data: freq=${freqSum}, time=${timeSum}, playing=${!audioPlayer.paused}`);
            }

            drawLiveWaveform(timeDomainData);
            drawLiveSpectrogram(frequencyData);

            animationId = requestAnimationFrame(startVisualizationLoop);
        }

        // Start simulation with fake data
        function startSimulation() {
            debugLog('🎬 Starting simulation...');
            isVisualizationActive = true;
            let frame = 0;
            
            function simulate() {
                if (!isVisualizationActive) return;
                
                frame++;
                
                // Generate fake data
                const fakeTime = new Uint8Array(1024);
                const fakeFreq = new Uint8Array(512);
                
                for (let i = 0; i < fakeTime.length; i++) {
                    fakeTime[i] = 128 + Math.sin(frame * 0.1 + i * 0.01) * 50;
                }
                
                for (let i = 0; i < fakeFreq.length; i++) {
                    fakeFreq[i] = Math.random() * 100 + Math.sin(frame * 0.05 + i * 0.02) * 80;
                }
                
                drawLiveWaveform(fakeTime);
                drawLiveSpectrogram(fakeFreq);
                
                requestAnimationFrame(simulate);
            }
            
            simulate();
        }

        // Draw live waveform
        function drawLiveWaveform(timeDomainData) {
            const canvas = waveformCanvas;
            const ctx = canvas.getContext('2d');
            
            canvas.width = canvas.parentElement.clientWidth - 40;
            canvas.height = canvas.parentElement.clientHeight - 40;

            ctx.fillStyle = '#1a202c';
            ctx.fillRect(0, 0, canvas.width, canvas.height);

            ctx.strokeStyle = '#00d4ff';
            ctx.lineWidth = 2;
            ctx.beginPath();

            const sliceWidth = canvas.width / timeDomainData.length;
            let x = 0;

            for (let i = 0; i < timeDomainData.length; i++) {
                const v = timeDomainData[i] / 128.0;
                const y = (v * canvas.height) / 2;

                if (i === 0) {
                    ctx.moveTo(x, y);
                } else {
                    ctx.lineTo(x, y);
                }
                x += sliceWidth;
            }

            ctx.stroke();
        }

        // Draw live spectrogram
        function drawLiveSpectrogram(frequencyData) {
            const canvas = spectrogramCanvas;
            const ctx = canvas.getContext('2d');
            
            canvas.width = canvas.parentElement.clientWidth - 40;
            canvas.height = canvas.parentElement.clientHeight - 40;

            // Shift existing data left
            if (canvas.width > 1) {
                const imageData = ctx.getImageData(1, 0, canvas.width - 1, canvas.height);
                ctx.putImageData(imageData, 0, 0);
            }

            // Draw new column
            const x = canvas.width - 1;
            
            for (let y = 0; y < canvas.height; y++) {
                const freqIndex = Math.floor((canvas.height - y) / canvas.height * frequencyData.length);
                const intensity = frequencyData[freqIndex] / 255.0;

                let r, g, b;
                if (intensity < 0.2) {
                    r = 0; g = 0; b = Math.floor(intensity * 5 * 255);
                } else if (intensity < 0.5) {
                    const t = (intensity - 0.2) / 0.3;
                    r = 0; g = Math.floor(t * 255); b = 255;
                } else if (intensity < 0.8) {
                    const t = (intensity - 0.5) / 0.3;
                    r = Math.floor(t * 255); g = 255; b = 255 - Math.floor(t * 255);
                } else {
                    const t = (intensity - 0.8) / 0.2;
                    r = 255; g = 255 - Math.floor(t * 128); b = 0;
                }

                ctx.fillStyle = `rgb(${r},${g},${b})`;
                ctx.fillRect(x, y, 1, 1);
            }
        }

        // Stop visualization
        function stopVisualization() {
            isVisualizationActive = false;
            if (animationId) {
                cancelAnimationFrame(animationId);
            }
            debugLog('🛑 Visualization stopped');
        }

        // Generate audio tokens
        function generateAudioTokens() {
            const duration = audioPlayer.duration || 4;
            const windowSize = 0.025;
            const hopSize = 0.01;
            const numWindows = Math.floor((duration - windowSize) / hopSize);
            const tokens = [];
            
            for (let i = 0; i < numWindows; i++) {
                const baseToken = 1000 + Math.floor(Math.random() * 8000);
                tokens.push(baseToken);
            }
            
            const displayTokens = tokens.slice(0, 60);
            const tokenString = `[${displayTokens.join(', ')}${tokens.length > 60 ? ', ...' : ''}]`;
            
            tokenArrayDiv.innerHTML = `
                <div style="margin-bottom: 10px;">
                    <strong>Audio Tokens (${tokens.length} total):</strong>
                </div>
                ${tokenString}
                <div style="margin-top: 15px; font-size: 12px; color: #a0aec0;">
                    • Duration: ${duration.toFixed(2)}s → ${tokens.length} tokens<br>
                    • Each token represents ~25ms of audio<br>
                    • Range: ${Math.min(...tokens)} - ${Math.max(...tokens)}
                </div>
            `;
            
            debugLog(`🔢 Generated ${tokens.length} tokens`);
        }

        // Update status
        function updateAudioStatus(message, isError = false) {
            const statusDiv = document.getElementById('audioStatus');
            statusDiv.textContent = message;
            statusDiv.style.color = isError ? '#dc3545' : '#666';
        }

        // Event Listeners
        audioPlayer.addEventListener('play', () => {
            debugLog('▶️ Audio play event');
            updateAudioStatus('🎵 Playing with live visualization');
            setTimeout(() => initLiveVisualization(), 100);
        });

        audioPlayer.addEventListener('pause', () => {
            debugLog('⏸️ Audio pause event');
            stopVisualization();
            updateAudioStatus('⏸️ Paused');
        });

        audioPlayer.addEventListener('ended', () => {
            debugLog('🔚 Audio ended');
            stopVisualization();
            updateAudioStatus('✅ Playback complete');
        });

        audioPlayer.addEventListener('canplay', () => {
            debugLog('✅ Audio ready');
            updateAudioStatus('✅ Audio ready! Click play to see live visualization');
        });

        audioPlayer.addEventListener('error', (e) => {
            debugLog(`❌ Audio error: ${e.type}`);
            updateAudioStatus('❌ Audio load failed', true);
        });

        testAudioBtn.addEventListener('click', testAudioSupport);

        generateTokensBtn.addEventListener('click', () => {
            debugLog('🔢 Generate tokens clicked');
            generateTokensBtn.disabled = true;
            generateTokensBtn.textContent = '🔄 Generating...';
            
            setTimeout(() => {
                generateAudioTokens();
                generateTokensBtn.textContent = '✅ Tokens Generated';
                
                setTimeout(() => {
                    generateTokensBtn.textContent = '🔢 Generate Tokens';
                    generateTokensBtn.disabled = false;
                }, 2000);
            }, 1000);
        });

        // Initialize
        window.addEventListener('load', () => {
            debugLog('🚀 Page loaded');
            
            // Start simulation after 3 seconds if no real audio
            setTimeout(() => {
                if (!isVisualizationActive) {
                    debugLog('🎬 Auto-starting simulation');
                    updateAudioStatus('🎬 Simulation mode (click play for real audio)');
                    startSimulation();
                }
            }, 3000);
        });
    </script>
</body>
</html>
